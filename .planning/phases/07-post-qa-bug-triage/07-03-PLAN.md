---
phase: 07-post-qa-bug-triage
plan: 03
type: execute
wave: 2
depends_on: ["07-02"]
files_modified:
  - src/aquamvs/benchmark/__init__.py
  - src/aquamvs/benchmark/runner.py
  - src/aquamvs/benchmark/metrics.py
  - src/aquamvs/benchmark/report.py
  - src/aquamvs/benchmark/config.py
  - src/aquamvs/benchmark/datasets.py
  - src/aquamvs/benchmark/comparison.py
  - src/aquamvs/benchmark/visualization.py
  - src/aquamvs/benchmark/synthetic.py
  - src/aquamvs/benchmark/synthetic_benchmark.py
  - src/aquamvs/cli.py
  - src/aquamvs/profiling/synthetic_profile.py
autonomous: true

must_haves:
  truths:
    - "aquamvs benchmark config.yaml runs all 4 pathways on frame 0 and prints a comparison table to console"
    - "aquamvs benchmark config.yaml --extractors superpoint,aliked,disk adds extractor variants for LightGlue paths"
    - "aquamvs benchmark config.yaml --with-clahe adds CLAHE-on variants for LightGlue paths"
    - "Each pathway row shows per-stage wall time (undistortion, matching, depth, fusion, surface), total time, point count, and outlier removal %"
    - "A markdown report is saved to {output_dir}/benchmark_{timestamp}.md"
    - "The old profile CLI command is removed — only benchmark remains"
    - "Old benchmark module files (config.py, datasets.py, comparison.py, visualization.py, synthetic.py, synthetic_benchmark.py) are deleted"
    - "Old profiling/synthetic_profile.py is deleted"
  artifacts:
    - path: "src/aquamvs/benchmark/runner.py"
      provides: "run_benchmark function that runs pathways and collects metrics"
      exports: ["run_benchmark", "BenchmarkResult", "PathwayResult"]
    - path: "src/aquamvs/benchmark/metrics.py"
      provides: "Relative metric computation (point count, outlier %)"
      exports: ["compute_relative_metrics"]
    - path: "src/aquamvs/benchmark/report.py"
      provides: "Console table and markdown report formatting"
      exports: ["format_console_table", "format_markdown_report"]
    - path: "src/aquamvs/benchmark/__init__.py"
      provides: "Clean public API for new benchmark module"
      exports: ["run_benchmark", "BenchmarkResult"]
    - path: "src/aquamvs/cli.py"
      provides: "New benchmark_command accepting PipelineConfig, --frame, --extractors, --with-clahe flags"
      contains: "--extractors"
  key_links:
    - from: "src/aquamvs/benchmark/runner.py"
      to: "src/aquamvs/profiling/profiler.py"
      via: "set_active_profiler wires profiler before each pathway run"
      pattern: "set_active_profiler"
    - from: "src/aquamvs/cli.py"
      to: "src/aquamvs/benchmark/runner.py"
      via: "benchmark_command calls run_benchmark"
      pattern: "run_benchmark"
    - from: "src/aquamvs/benchmark/runner.py"
      to: "src/aquamvs/pipeline/runner.py"
      via: "process_frame called for each pathway"
      pattern: "process_frame"
---

<objective>
Rebuild the benchmark command as a unified pathway comparison tool replacing both the old benchmark and profile commands.

Purpose: The old `aquamvs benchmark` expects a BenchmarkConfig with synthetic/ChArUco datasets (broken). The old `aquamvs profile` has an empty report table (fixed by Plan 02 but still separate). Per user decision: combine into one `aquamvs benchmark` command that takes a PipelineConfig, runs all 4 execution paths, and reports per-stage timing + relative accuracy metrics.

Output: Working `aquamvs benchmark config.yaml` command with console table + markdown report.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-post-qa-bug-triage/07-RESEARCH.md
@.planning/phases/07-post-qa-bug-triage/07-02-SUMMARY.md

@src/aquamvs/benchmark/__init__.py
@src/aquamvs/benchmark/runner.py
@src/aquamvs/cli.py
@src/aquamvs/profiling/profiler.py
@src/aquamvs/pipeline/runner.py
@src/aquamvs/pipeline/builder.py
@src/aquamvs/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Delete old benchmark/profiler code, build new benchmark module</name>
  <files>
    src/aquamvs/benchmark/__init__.py,
    src/aquamvs/benchmark/runner.py,
    src/aquamvs/benchmark/metrics.py,
    src/aquamvs/benchmark/report.py,
    src/aquamvs/benchmark/config.py (DELETE),
    src/aquamvs/benchmark/datasets.py (DELETE),
    src/aquamvs/benchmark/comparison.py (DELETE),
    src/aquamvs/benchmark/visualization.py (DELETE),
    src/aquamvs/benchmark/synthetic.py (DELETE),
    src/aquamvs/benchmark/synthetic_benchmark.py (DELETE),
    src/aquamvs/profiling/synthetic_profile.py (DELETE)
  </files>
  <action>
**Delete old files** — Remove these files entirely (they are part of the old BenchmarkConfig/dataset model):
- `src/aquamvs/benchmark/config.py`
- `src/aquamvs/benchmark/datasets.py`
- `src/aquamvs/benchmark/comparison.py`
- `src/aquamvs/benchmark/visualization.py`
- `src/aquamvs/benchmark/synthetic.py`
- `src/aquamvs/benchmark/synthetic_benchmark.py`
- `src/aquamvs/profiling/synthetic_profile.py`

**New benchmark/runner.py** — Core benchmark runner. Key elements:

```python
"""Benchmark runner for comparing pipeline execution pathways."""

from dataclasses import dataclass, field
from pathlib import Path

from ..config import PipelineConfig
from ..profiling.profiler import PipelineProfiler, set_active_profiler
from ..profiling.analyzer import ProfileReport


@dataclass
class PathwayResult:
    """Results from a single pathway execution."""
    pathway_name: str
    timing: ProfileReport
    point_count: int = 0
    outlier_removal_pct: float = 0.0
    # Stage-specific info
    stages_run: list[str] = field(default_factory=list)


@dataclass
class BenchmarkResult:
    """Results from a complete benchmark run."""
    results: list[PathwayResult] = field(default_factory=list)
    config_path: str = ""
    frame: int = 0


def build_pathways(
    base_config: PipelineConfig,
    extractors: list[str] | None = None,
    with_clahe: bool = False,
) -> list[tuple[str, PipelineConfig]]:
    """Build list of (name, config) tuples for each pathway to benchmark.

    Always includes 4 base pathways:
    1. LG+SP sparse — LightGlue + SuperPoint, pipeline_mode=sparse
    2. LG+SP full — LightGlue + SuperPoint, pipeline_mode=full
    3. RoMa sparse — RoMa, pipeline_mode=sparse
    4. RoMa full — RoMa, pipeline_mode=full

    With --extractors: adds LightGlue variants per extractor
    With --with-clahe: adds CLAHE-on variants for LightGlue paths
    """
    # Build pathways by deep-copying base_config and modifying:
    # - config.matcher_type = "lightglue" | "roma"
    # - config.pipeline_mode = "sparse" | "full"
    # - config.sparse_matching.extractor = extractor_name
    # - config.preprocessing.color_norm_enabled = True/False for CLAHE
    ...


def run_benchmark(
    config_path: Path,
    frame: int = 0,
    extractors: list[str] | None = None,
    with_clahe: bool = False,
) -> BenchmarkResult:
    """Run benchmark comparison across all pathways.

    For each pathway:
    1. Deep-copy and modify config for the pathway
    2. Set active profiler (from Plan 02's thread-local registry)
    3. Read frame, build context, run process_frame
    4. Collect profiler report + relative metrics
    5. Deactivate profiler

    Uses PipelineConfig.from_yaml to load config (not BenchmarkConfig).
    Uses set_active_profiler/get_active_profiler from profiling.profiler.
    """
    ...
```

For reading a single frame: use the same pattern as `profile_pipeline()` — detect input type, open source, read_frame(frame_idx). Factor this into a helper `_read_single_frame(config, frame) -> dict[str, np.ndarray]`.

For collecting metrics: after process_frame, count points in fused cloud (glob for `fused_points.ply` in frame output dir), and parse outlier removal % from log output or compute from before/after counts if available. If point cloud doesn't exist (sparse-only paths), report 0.

Each pathway runs in a temporary output directory (or a subdirectory of the main output dir) to avoid overwriting previous pathway results. Use `{output_dir}/benchmark/{pathway_name}/` as the working directory for each pathway.

**New benchmark/metrics.py** — Relative metric computation:

```python
"""Relative accuracy metrics for benchmark comparison."""

def compute_relative_metrics(pathway_output_dir: Path) -> dict[str, float]:
    """Compute relative metrics from a pathway's output.

    Returns dict with:
    - point_count: number of points in fused cloud (0 if no cloud)
    - outlier_removal_pct: percentage of points removed as outliers
    """
    ...
```

Look for `fused_points.ply` in the pathway output dir. Use Open3D to load and count points. For outlier %, look for the raw cloud before outlier removal vs after, or parse from the fusion stage log. Keep it simple — if data isn't available, report 0 or N/A.

**New benchmark/report.py** — Format results for console and markdown:

```python
"""Benchmark report formatting (console table and markdown file)."""

def format_console_table(result: BenchmarkResult) -> str:
    """Format benchmark results as a pretty-printed console table.

    Columns: Pathway | Undist (s) | Match (s) | Depth (s) | Fusion (s) | Surface (s) | Total (s) | Points | Outlier %

    Use tabulate library (already in deps) for formatting.
    Stages not run by a pathway (e.g., depth/fusion in sparse mode) show "—".
    Times in seconds with 1 decimal place.
    """
    ...

def format_markdown_report(result: BenchmarkResult) -> str:
    """Format benchmark results as a markdown report.

    Includes:
    - Header with config path, frame index, timestamp
    - Results table (same layout as console)
    - System info (device, CUDA version if applicable)
    """
    ...

def save_markdown_report(result: BenchmarkResult, output_dir: Path) -> Path:
    """Save markdown report to output_dir/benchmark_YYYYMMDD_HHMMSS.md."""
    ...
```

**New benchmark/__init__.py** — Clean public API:

```python
"""Benchmark tools for comparing pipeline execution pathways."""

from .runner import run_benchmark, BenchmarkResult, PathwayResult
from .report import format_console_table, format_markdown_report, save_markdown_report

__all__ = [
    "run_benchmark",
    "BenchmarkResult",
    "PathwayResult",
    "format_console_table",
    "format_markdown_report",
    "save_markdown_report",
]
```

Architecture note: the benchmark module is structured so synthetic data support could be added later by providing an alternative `_read_single_frame` implementation. But do NOT implement synthetic support — it's deferred.
  </action>
  <verify>
1. `python -c "from aquamvs.benchmark import run_benchmark, BenchmarkResult, format_console_table"` — imports work
2. Deleted files no longer exist: `ls src/aquamvs/benchmark/config.py 2>&1` should show "No such file"
3. `python -c "from aquamvs.benchmark.runner import build_pathways"` — pathway builder importable
4. `python -c "from aquamvs.benchmark.report import format_console_table, format_markdown_report, save_markdown_report"` — report functions importable
  </verify>
  <done>Old benchmark module gutted and rebuilt. New module takes PipelineConfig, runs 4+ pathways with profiler wiring, collects timing + relative metrics. Old synthetic/dataset code deleted.</done>
</task>

<task type="auto">
  <name>Task 2: Replace CLI benchmark/profile commands with new unified benchmark</name>
  <files>src/aquamvs/cli.py</files>
  <action>
**Remove profile CLI command** — Delete:
1. The `profile_parser` argparse setup (lines ~748-769 in cli.py)
2. The `profile_command()` function entirely
3. The `elif args.command == "profile"` dispatch block

**Replace benchmark CLI command** — Replace the old `benchmark_parser` and `benchmark_command()` with the new design:

1. New argparse setup for benchmark:
```python
benchmark_parser = subparsers.add_parser(
    "benchmark",
    help="Compare pipeline pathways (timing and accuracy metrics)",
)
benchmark_parser.add_argument(
    "config",
    type=Path,
    help="Path to pipeline config YAML file (same as 'aquamvs run')",
)
benchmark_parser.add_argument(
    "--frame",
    type=int,
    default=0,
    help="Frame index to benchmark (default: 0)",
)
benchmark_parser.add_argument(
    "--extractors",
    type=str,
    default=None,
    help="Comma-separated list of extractors for LightGlue paths (e.g., superpoint,aliked,disk)",
)
benchmark_parser.add_argument(
    "--with-clahe",
    action="store_true",
    help="Include CLAHE preprocessing variants for LightGlue paths",
)
```

2. New `benchmark_command()`:
```python
def benchmark_command(args) -> None:
    """Run pathway comparison benchmark."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%H:%M:%S",
    )
    # Silence noisy loggers
    for name in ("matplotlib", "PIL", "open3d"):
        logging.getLogger(name).setLevel(logging.WARNING)

    config_path = args.config
    if not config_path.exists():
        print(f"Error: Config file not found: {config_path}", file=sys.stderr)
        sys.exit(1)

    # Parse extractors
    extractors = None
    if args.extractors:
        extractors = [e.strip() for e in args.extractors.split(",")]

    from aquamvs.benchmark import run_benchmark, format_console_table, save_markdown_report

    try:
        result = run_benchmark(
            config_path=config_path,
            frame=args.frame,
            extractors=extractors,
            with_clahe=args.with_clahe,
        )

        # Print console table
        print(format_console_table(result))

        # Save markdown report
        report_path = save_markdown_report(result, Path(result.config_path).parent)
        print(f"\nReport saved to: {report_path}")

    except Exception as e:
        print(f"Error: Benchmark failed: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)
```

3. Update dispatch:
```python
elif args.command == "benchmark":
    benchmark_command(args)
```

**Update any tests** that reference the old benchmark_command signature or profile_command. Search for imports of `benchmark_command`, `profile_command`, `BenchmarkConfig` from aquamvs in test files and update or remove them. Also check `tests/test_ci_benchmarks.py` — this was flagged as having stale references from Phase 05.
  </action>
  <verify>
1. `aquamvs benchmark --help` shows config (positional), --frame, --extractors, --with-clahe flags
2. `aquamvs profile --help 2>&1` should fail (command removed)
3. `aquamvs --help` lists benchmark but NOT profile
4. `python -c "from aquamvs.cli import benchmark_command"` — new function importable
5. `grep -r "profile_command\|BenchmarkConfig" src/aquamvs/cli.py` — no references to old code
6. `pytest tests/ -x -q --timeout=60 -k "benchmark or profile or cli"` — tests pass (with stale test fixes)
  </verify>
  <done>CLI has single unified `aquamvs benchmark` command. Old `profile` command removed. Old `benchmark_command` with BenchmarkConfig replaced. Stale test references cleaned up.</done>
</task>

</tasks>

<verification>
1. `aquamvs benchmark --help` shows the new flags (--frame, --extractors, --with-clahe)
2. `aquamvs profile` fails with "invalid choice" (command removed)
3. Old benchmark files deleted: config.py, datasets.py, comparison.py, visualization.py, synthetic.py, synthetic_benchmark.py
4. `python -c "from aquamvs.benchmark import run_benchmark, BenchmarkResult"` — new API importable
5. `pytest tests/ -x -q --timeout=60` — all tests pass
6. No references to BenchmarkConfig, profile_command, or synthetic_profile in src/aquamvs/
</verification>

<success_criteria>
- `aquamvs benchmark config.yaml` runs 4 pathways, prints comparison table, saves markdown report
- `--extractors` and `--with-clahe` flags add additional pathway variants
- Old profile command fully removed
- Old benchmark module fully replaced (no BenchmarkConfig, no datasets, no synthetic code)
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-post-qa-bug-triage/07-03-SUMMARY.md`
</output>
