---
phase: 08-user-guide-and-tutorials-overhaul
plan: 03
type: execute
wave: 2
depends_on: ["08-01", "08-02"]
files_modified:
  - docs/tutorial/benchmark.ipynb
  - docs/benchmarks.rst
  - docs/index.rst
  - tests/test_notebook_smoke.py
autonomous: true

must_haves:
  truths:
    - "Benchmarking notebook demonstrates full workflow: run benchmark, load results, compare pathways, visualize metrics"
    - "Benchmarking notebook has Colab badge and auto-downloads example dataset"
    - "benchmarks.rst redirects to the benchmark notebook instead of documenting removed commands"
    - "Smoke test validates notebook code cells parse as valid Python and import statements resolve"
    - "docs/index.rst toctree includes troubleshooting page"
  artifacts:
    - path: "docs/tutorial/benchmark.ipynb"
      provides: "Benchmarking tutorial notebook"
    - path: "docs/benchmarks.rst"
      provides: "Redirect page pointing to benchmark notebook"
    - path: "tests/test_notebook_smoke.py"
      provides: "Smoke test for notebook syntax and import resolution"
    - path: "docs/index.rst"
      provides: "Updated toctree with troubleshooting"
  key_links:
    - from: "docs/tutorial/benchmark.ipynb"
      to: "aquamvs.benchmark"
      via: "Python import in code cell"
      pattern: "from aquamvs"
    - from: "docs/benchmarks.rst"
      to: "docs/tutorial/benchmark.ipynb"
      via: "cross-reference directive"
      pattern: "tutorial/benchmark"
    - from: "tests/test_notebook_smoke.py"
      to: "docs/**/*.ipynb"
      via: "glob discovery"
      pattern: "rglob.*ipynb"
---

<objective>
Create the benchmarking notebook, replace the stale benchmarks.rst, update the docs toctree, and add notebook smoke tests.

Purpose: Provide a hands-on benchmarking tutorial comparing reconstruction pathways with visualizations, retire the outdated benchmarks docs page, and add CI protection against notebook code rot.
Output: New benchmark.ipynb, updated benchmarks.rst, updated index.rst, new test_notebook_smoke.py
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-user-guide-and-tutorials-overhaul/08-RESEARCH.md
@.planning/phases/08-user-guide-and-tutorials-overhaul/08-01-SUMMARY.md
@docs/benchmarks.rst
@docs/index.rst
@src/aquamvs/benchmark/runner.py
@src/aquamvs/benchmark/report.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create benchmarking notebook and replace benchmarks.rst</name>
  <files>docs/tutorial/benchmark.ipynb, docs/benchmarks.rst, docs/index.rst</files>
  <action>
  **docs/tutorial/benchmark.ipynb — Create new notebook:**

  Structure (all cells, in order):

  1. **Markdown: Colab badge + title**
     ```html
     <a href="https://colab.research.google.com/github/tlancaster6/AquaMVS/blob/main/docs/tutorial/benchmark.ipynb">
       <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
     </a>

     # Benchmarking Reconstruction Pathways

     This tutorial demonstrates how to benchmark AquaMVS reconstruction pathways,
     compare LightGlue and RoMa matching strategies, and visualize performance
     and quality metrics.

     AquaMVS supports four reconstruction pathways:
     - **LightGlue + Sparse**: Fast sparse reconstruction using SuperPoint features
     - **LightGlue + Full**: Dense stereo seeded by LightGlue sparse matches
     - **RoMa + Sparse**: Sparse reconstruction using dense RoMa correspondences
     - **RoMa + Full**: Dense stereo seeded by RoMa dense matches
     ```

  2. **Code: Colab install** (same pattern as API notebook)
     ```python
     import subprocess, sys
     try:
         import aquamvs
     except ImportError:
         subprocess.run([sys.executable, "-m", "pip", "install", "torch", "torchvision",
                         "--index-url", "https://download.pytorch.org/whl/cpu", "-q"], check=True)
         subprocess.run([sys.executable, "-m", "pip", "install",
                         "git+https://github.com/cvg/LightGlue.git@edb2b83",
                         "git+https://github.com/tlancaster6/RoMaV2.git",
                         "aquamvs", "-q"], check=True)
     ```

  3. **Code: Auto-download dataset** (same pattern as API notebook)
     ```python
     import os, urllib.request, zipfile
     from pathlib import Path

     DATASET_URL = "https://github.com/tlancaster6/AquaMVS/releases/download/v0.1.0-example-data/aquamvs-example-dataset.zip"
     DATASET_DIR = Path("aquamvs-example-dataset")

     if not DATASET_DIR.exists():
         print("Downloading example dataset...")
         urllib.request.urlretrieve(DATASET_URL, "aquamvs-example-dataset.zip")
         with zipfile.ZipFile("aquamvs-example-dataset.zip") as zf:
             zf.extractall()
         os.remove("aquamvs-example-dataset.zip")
         print("Done.")
     else:
         print(f"Dataset already present at {DATASET_DIR}")
     ```

  4. **Markdown: "Running the Benchmark"** — Explain what `aquamvs benchmark` does and how it compares 4 pathways.

  5. **Code: Run benchmark via CLI** (shell out, since the CLI is the primary interface per Phase 7)
     ```python
     import subprocess, sys
     result = subprocess.run(
         [sys.executable, "-m", "aquamvs", "benchmark",
          str(DATASET_DIR / "config.yaml"), "--frame", "0"],
         capture_output=True, text=True
     )
     print(result.stdout)
     if result.returncode != 0:
         print("STDERR:", result.stderr)
     ```

  6. **Markdown: "Loading Results Programmatically"** — Show how to access benchmark results via the Python API for custom analysis.

  7. **Code: Load and inspect results**
     Read the benchmark runner module to determine the actual return types and API. Use the benchmark Python API:
     ```python
     from aquamvs.benchmark.runner import run_benchmark
     from aquamvs.config import PipelineConfig

     config = PipelineConfig.from_yaml(DATASET_DIR / "config.yaml")
     result = run_benchmark(config_path=DATASET_DIR / "config.yaml", frame=0)

     for pw in result.results:
         print(f"{pw.pathway_name}: {pw.timing.total_time_ms/1000:.1f}s, "
               f"{pw.point_count} points, density={pw.cloud_density:.0f} pts/m²")
     ```
     **IMPORTANT:** Check the actual API in `src/aquamvs/benchmark/runner.py` and `src/aquamvs/benchmark/report.py` to get the correct attribute names. The research file's code examples are approximations — use the real API.

  8. **Markdown: "Visualizing Results"** — Introduce the comparison charts.

  9. **Code: Total runtime bar chart** — matplotlib bar chart comparing total time per pathway. Use research file's code pattern (axes[0] for time, axes[1] for point count). Colors: professional palette.

  10. **Markdown: "Stage Timing Breakdown"** — Explain what stage timings show.

  11. **Code: Stacked bar chart of stage timings** — Use research file's stacked bar pattern. Stages: check actual stage names from profiler (undistortion, sparse_matching, depth_estimation, fusion, surface or similar). Use the real attribute path from the profiler report.

  12. **Markdown: "Comparing Depth Maps"** — Show depth maps side by side for visual quality comparison.

  13. **Code: Side-by-side depth map comparison** — Load depth maps from two pathways (e.g., LightGlue full vs RoMa full), display with matplotlib subplots. Use `output / pathway_name / "depth_maps" / f"{cam}.npz"` paths (check actual benchmark output structure).
      If benchmark output structure doesn't save per-pathway depth maps separately, skip this cell and note that visual comparison requires running each pathway individually.

  14. **Markdown: "Selecting a Pathway"** — Summary table and guidance on when to use each pathway.

  15. **Markdown: "Next Steps"** — Link to CLI guide, API tutorial, troubleshooting. Professional/academic tone.

  Write notebook code with empty outputs for now. Pre-executed outputs will be added in Plan 04 (checkpoint task where user executes notebooks on real hardware and commits the outputs). Per locked decision: notebooks are committed with cell outputs already executed so RTD renders visible results.

  **docs/benchmarks.rst — Replace content:**
  Replace the entire file with a short redirect page:
  ```rst
  Benchmark Results
  =================

  The benchmark documentation has moved. AquaMVS benchmarking is now covered in the
  interactive tutorial notebook:

  - :doc:`Benchmarking Tutorial <tutorial/benchmark>` — Compare LightGlue and RoMa
    reconstruction pathways with timing, quality metrics, and visualizations.

  For CLI usage, run:

  .. code-block:: bash

      aquamvs benchmark config.yaml --frame 0

  See the :doc:`CLI Guide <cli_guide>` for details on command-line options.
  ```

  **docs/index.rst — Update toctree:**
  Add `troubleshooting` to the "User Guide" toctree (after `benchmarks`):
  ```rst
  .. toctree::
     :maxdepth: 2
     :caption: User Guide

     tutorial/index
     cli_guide
     benchmarks
     troubleshooting
  ```
  </action>
  <verify>
  - `python -c "import json; nb=json.load(open('docs/tutorial/benchmark.ipynb')); print(len(nb['cells']), 'cells')"` shows 12+ cells
  - First cell contains Colab badge HTML
  - `grep "tutorial/benchmark" docs/benchmarks.rst` finds the redirect
  - `grep "troubleshooting" docs/index.rst` finds the toctree entry
  - `grep -c "aquamvs profile" docs/benchmarks.rst` returns 0 (removed command)
  </verify>
  <done>Benchmark notebook covers full workflow (run, load, visualize). benchmarks.rst redirects to notebook. index.rst toctree includes troubleshooting.</done>
</task>

<task type="auto">
  <name>Task 2: Add notebook smoke test</name>
  <files>tests/test_notebook_smoke.py</files>
  <action>
  Create `tests/test_notebook_smoke.py` with import-only smoke testing for all notebooks.

  Per locked decision: import-only smoke test — verify notebook code cells parse and imports resolve, no full execution.

  ```python
  """Smoke tests for Jupyter notebooks.

  Validates that all code cells in documentation notebooks:
  1. Parse as valid Python (syntax check)
  2. Have resolvable import statements (import check)

  Does NOT execute notebooks (no dataset or GPU required).
  """

  import ast
  import importlib
  import json
  from pathlib import Path

  import pytest

  NOTEBOOK_PATHS = sorted(Path("docs").rglob("*.ipynb"))

  # Modules that require hardware/dataset and should be skipped in import check
  SKIP_IMPORT_MODULES = {"google.colab"}


  def _extract_imports(source: str) -> list[str]:
      """Extract top-level module names from import statements."""
      try:
          tree = ast.parse(source)
      except SyntaxError:
          return []
      modules = []
      for node in ast.walk(tree):
          if isinstance(node, ast.Import):
              for alias in node.names:
                  modules.append(alias.name.split(".")[0])
          elif isinstance(node, ast.ImportFrom) and node.module:
              modules.append(node.module.split(".")[0])
      return modules


  @pytest.mark.parametrize("nb_path", NOTEBOOK_PATHS, ids=lambda p: p.stem)
  def test_notebook_syntax(nb_path):
      """All notebook code cells must parse as valid Python."""
      with open(nb_path) as f:
          nb = json.load(f)
      for i, cell in enumerate(nb["cells"]):
          if cell["cell_type"] != "code":
              continue
          source = "".join(cell["source"])
          if not source.strip():
              continue
          # Skip cells that are shell commands (start with !)
          if source.strip().startswith("!"):
              continue
          try:
              ast.parse(source)
          except SyntaxError as e:
              pytest.fail(f"{nb_path.name} cell {i}: {e}")


  @pytest.mark.parametrize("nb_path", NOTEBOOK_PATHS, ids=lambda p: p.stem)
  def test_notebook_imports(nb_path):
      """All import statements in notebook code cells must resolve."""
      with open(nb_path) as f:
          nb = json.load(f)
      for i, cell in enumerate(nb["cells"]):
          if cell["cell_type"] != "code":
              continue
          source = "".join(cell["source"])
          if not source.strip() or source.strip().startswith("!"):
              continue
          for mod in _extract_imports(source):
              if mod in SKIP_IMPORT_MODULES:
                  continue
              try:
                  importlib.import_module(mod)
              except ImportError:
                  pytest.fail(
                      f"{nb_path.name} cell {i}: cannot import '{mod}'"
                  )
  ```

  Notes:
  - `test_notebook_syntax`: Uses `ast.parse()` to validate Python syntax
  - `test_notebook_imports`: Extracts import statements via AST and calls `importlib.import_module()` on top-level module names to verify they resolve
  - Skips empty cells and shell command cells (lines starting with `!`)
  - `SKIP_IMPORT_MODULES` allowlist for modules not available in CI (e.g., google.colab)
  - Parametrized by notebook path so failures identify which notebook
  - Located in top-level tests/ directory (not docs/)
  - No extra dependencies beyond stdlib + pytest
  </action>
  <verify>
  - `pytest tests/test_notebook_smoke.py -v` passes (all notebook cells parse and imports resolve)
  - Test discovers both `notebook.ipynb` and `benchmark.ipynb`
  - Both `test_notebook_syntax` and `test_notebook_imports` parametrized tests appear in output
  - Intentionally broken Python in a cell would cause test failure (sanity check: temporarily corrupt a cell and verify failure)
  </verify>
  <done>Smoke test validates all notebook code cells parse as valid Python and all import statements resolve. Test runs in existing CI without dataset or GPU.</done>
</task>

</tasks>

<verification>
- `pytest tests/test_notebook_smoke.py -v` passes for all notebooks
- `sphinx-build -W -b html docs/ docs/_build/html` builds successfully with benchmark notebook rendered
- benchmarks.rst no longer documents removed commands (aquamvs profile, benchmark YAML config)
- docs/index.rst toctree includes troubleshooting entry
- Benchmark notebook contains Colab badge, auto-download, visualization code
</verification>

<success_criteria>
Benchmarking notebook provides full workflow from running benchmark to visualizing results. benchmarks.rst redirects to notebook. Smoke test catches syntax errors in all notebooks. docs toctree is complete with troubleshooting page.
</success_criteria>

<output>
After completion, create `.planning/phases/08-user-guide-and-tutorials-overhaul/08-03-SUMMARY.md`
</output>
