---
phase: 02-configuration-and-api-cleanup
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/aquamvs/config.py
  - src/aquamvs/__init__.py
  - tests/test_config.py
autonomous: true

must_haves:
  truths:
    - "Invalid config files produce clear error messages identifying all problems with YAML paths (not fail-on-first)"
    - "Config structure consolidated from 14 dataclasses to ~6 Pydantic models grouped by pipeline stage"
    - "User can create minimal config file with only paths + matcher_type and all defaults fill in"
    - "Unknown YAML keys produce a warning log, not a validation error (forward-compatible)"
    - "Cross-stage constraints validated at load time (e.g., matcher_type=roma implies dense_matching settings)"
    - "Existing YAML config files load without error (backward compatible field names)"
    - "Loading config with missing optional sections logs applied defaults at INFO level"
  artifacts:
    - path: "pyproject.toml"
      provides: "pydantic>=2.12.0 and tqdm>=4.66.0 in dependencies"
    - path: "src/aquamvs/config.py"
      provides: "PreprocessingConfig, SparseMatchingConfig, DenseMatchingConfig, ReconstructionConfig, RuntimeConfig, PipelineConfig as Pydantic BaseModels"
    - path: "tests/test_config.py"
      provides: "Tests for Pydantic config models, validation, YAML round-trip, error collection, unknown keys warning"
  key_links:
    - from: "src/aquamvs/config.py"
      to: "pydantic"
      via: "BaseModel, Field, ConfigDict, model_validator, field_validator"
    - from: "src/aquamvs/config.py"
      to: "pyyaml"
      via: "from_yaml() and to_yaml() class methods on PipelineConfig"
    - from: "tests/test_config.py"
      to: "src/aquamvs/config.py"
      via: "Import and test all Pydantic model classes"
---

<objective>
Migrate AquaMVS configuration from 14 Python dataclasses to ~6 validated Pydantic v2 models grouped by pipeline stage. Add load-time validation with error collection, unknown-key warnings, cross-stage constraint checking, and default logging.

Purpose: Config errors currently fail on first error with vague messages. Pydantic provides automatic error collection with YAML-path error messages, type coercion, and cross-field validation. Grouping by pipeline stage reduces cognitive overhead from 14 separate config classes to 5-6 logical groups.

Output: Rewritten `config.py` with Pydantic BaseModels, updated `pyproject.toml` with new dependencies, and comprehensive tests.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-configuration-and-api-cleanup/02-RESEARCH.md
@.planning/phases/02-configuration-and-api-cleanup/02-CONTEXT.md
@src/aquamvs/config.py
@src/aquamvs/__init__.py
@tests/test_config.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dependencies and rewrite config.py with Pydantic models</name>
  <files>
    pyproject.toml
    src/aquamvs/config.py
    src/aquamvs/__init__.py
  </files>
  <action>
1. **Add dependencies to `pyproject.toml`**:
   - Add `pydantic>=2.12.0` to `dependencies` list
   - Add `tqdm>=4.66.0` to `dependencies` list (used in plan 02, but adding now to avoid partial installs)
   - Run `pip install -e ".[dev]"` to install new deps

2. **Rewrite `src/aquamvs/config.py`** from dataclasses to Pydantic BaseModel. The new structure groups 14 dataclasses into ~6 Pydantic models by pipeline stage:

   **Top-level field mapping (PipelineConfig):**
   - Required: `calibration_path: str`, `output_dir: str`, `camera_video_map: dict[str, str]`
   - Required: `matcher_type: Literal["lightglue", "roma"]` (with Literal for enum validation)
   - Optional: `mask_dir: str | None = None`, `pipeline_mode: Literal["sparse", "full"] = "full"`
   - Sub-configs: `preprocessing`, `sparse_matching`, `dense_matching`, `reconstruction`, `runtime`

   **PreprocessingConfig** (absorbs ColorNormConfig + FrameSamplingConfig):
   ```python
   class PreprocessingConfig(BaseModel):
       model_config = ConfigDict(extra='allow')
       # Color normalization
       color_norm_enabled: bool = False
       color_norm_method: Literal["gain", "histogram"] = "gain"
       # Frame sampling
       frame_start: int = 0
       frame_stop: int | None = None
       frame_step: int = 1
   ```

   **SparseMatchingConfig** (absorbs FeatureExtractionConfig + PairSelectionConfig + MatchingConfig):
   ```python
   class SparseMatchingConfig(BaseModel):
       model_config = ConfigDict(extra='allow')
       # Feature extraction
       extractor_type: Literal["superpoint", "aliked", "disk"] = "superpoint"
       max_keypoints: int = 2048
       detection_threshold: float = 0.005
       clahe_enabled: bool = False
       clahe_clip_limit: float = 2.0
       # Pair selection
       num_neighbors: int = 4
       include_center: bool = True
       # Matching
       filter_threshold: float = 0.1
   ```

   **DenseMatchingConfig** (stays as-is, already a clean single config):
   ```python
   class DenseMatchingConfig(BaseModel):
       model_config = ConfigDict(extra='allow')
       certainty_threshold: float = 0.5
       max_correspondences: int = 100000
   ```

   **ReconstructionConfig** (absorbs DenseStereoConfig + FusionConfig + SurfaceConfig + OutlierRemovalConfig):
   ```python
   class ReconstructionConfig(BaseModel):
       model_config = ConfigDict(extra='allow')
       # Dense stereo
       num_depths: int = 128
       cost_function: Literal["ncc", "ssim"] = "ncc"
       window_size: int = 11
       depth_margin: float = 0.05
       # Fusion
       min_consistent_views: int = 3
       depth_tolerance: float = 0.005
       roma_depth_tolerance: float = 0.02
       voxel_size: float = 0.001
       min_confidence: float = 0.1
       # Surface
       surface_method: Literal["poisson", "heightfield", "bpa"] = "poisson"
       poisson_depth: int = 9
       grid_resolution: float = 0.002
       bpa_radii: list[float] | None = None
       target_faces: int | None = None
       # Outlier removal
       outlier_removal_enabled: bool = True
       outlier_nb_neighbors: int = 20
       outlier_std_ratio: float = 2.0
   ```

   **RuntimeConfig** (absorbs DeviceConfig + OutputConfig + VizConfig + BenchmarkConfig + EvaluationConfig):
   ```python
   class RuntimeConfig(BaseModel):
       model_config = ConfigDict(extra='allow')
       # Device
       device: Literal["cpu", "cuda"] = "cpu"
       # Output
       save_features: bool = False
       save_depth_maps: bool = True
       save_point_cloud: bool = True
       save_mesh: bool = True
       keep_intermediates: bool = True
       save_consistency_maps: bool = False
       # Visualization
       viz_enabled: bool = False
       viz_stages: list[str] = Field(default_factory=list)
       # Benchmark
       benchmark_extractors: list[str] = Field(default_factory=lambda: ["superpoint", "aliked", "disk"])
       benchmark_clahe: list[bool] = Field(default_factory=lambda: [True, False])
       # Evaluation
       icp_max_distance: float = 0.01
       # Progress
       quiet: bool = False
   ```

   **PipelineConfig** (top-level):
   ```python
   class PipelineConfig(BaseModel):
       model_config = ConfigDict(extra='allow')
       # Required (no defaults)
       calibration_path: str = ""
       output_dir: str = ""
       camera_video_map: dict[str, str] = Field(default_factory=dict)
       # Optional with defaults
       mask_dir: str | None = None
       pipeline_mode: Literal["sparse", "full"] = "full"
       matcher_type: Literal["lightglue", "roma"] = "lightglue"
       # Sub-configs
       preprocessing: PreprocessingConfig = Field(default_factory=PreprocessingConfig)
       sparse_matching: SparseMatchingConfig = Field(default_factory=SparseMatchingConfig)
       dense_matching: DenseMatchingConfig = Field(default_factory=DenseMatchingConfig)
       reconstruction: ReconstructionConfig = Field(default_factory=ReconstructionConfig)
       runtime: RuntimeConfig = Field(default_factory=RuntimeConfig)
   ```

3. **Field validators on each sub-config** (use `@field_validator`):
   - `ReconstructionConfig`: `window_size` must be positive and odd
   - `RuntimeConfig`: `viz_stages` entries must be in `VALID_VIZ_STAGES`
   - `RuntimeConfig`: `benchmark_extractors` entries must be in `VALID_EXTRACTORS`

4. **Cross-stage model_validator on PipelineConfig** (use `@model_validator(mode='after')`):
   - If `matcher_type == "roma"` and `dense_matching.certainty_threshold < 0.1`, warn (not error — research says validate cross-stage constraints)
   - Validation happens automatically via Pydantic Literal types for enum fields

5. **Extra fields warning on ALL models** via a shared `@model_validator(mode='after')`:
   - Check `self.__pydantic_extra__` and log warnings for unknown keys
   - Use `logger.warning("Unknown config keys in %s (ignored): %s", cls_name, keys)`

6. **`from_yaml()` class method** on PipelineConfig:
   - Load YAML with `yaml.safe_load()`
   - **Backward compatibility mapping**: Before passing to `model_validate()`, remap old flat field names to new nested structure. Check for old-style keys at top level:
     - `color_norm` → `preprocessing.color_norm_enabled`, `preprocessing.color_norm_method`
     - `frame_sampling` → `preprocessing.frame_start`, etc.
     - `feature_extraction` → `sparse_matching.extractor_type`, etc.
     - `pair_selection` → `sparse_matching.num_neighbors`, etc.
     - `matching` → `sparse_matching.filter_threshold`
     - `dense_stereo` → `reconstruction.num_depths`, etc.
     - `fusion` → `reconstruction.min_consistent_views`, etc.
     - `surface` → `reconstruction.surface_method`, etc.
     - `outlier_removal` → `reconstruction.outlier_removal_enabled`, etc.
     - `device` → `runtime.device`
     - `output` → `runtime.save_features`, etc.
     - `visualization` → `runtime.viz_enabled`, etc.
     - `benchmark` → `runtime.benchmark_extractors`, etc.
     - `evaluation` → `runtime.icp_max_distance`
   - When old-style keys found, log `logger.info("Migrating legacy config key '%s' to new structure", key)`
   - Log INFO for each missing optional section: "Using default: preprocessing (all defaults)"
   - Call `model_validate(data)` — Pydantic collects all errors
   - Return the validated config
   - Wrap the whole thing: catch `ValidationError`, call `format_validation_errors()` to format with YAML paths, then re-raise as `ValueError` with the formatted message

7. **`to_yaml()` method** on PipelineConfig:
   - Use `model_dump()` (Pydantic v2 replacement for `.dict()`)
   - Write with `yaml.safe_dump(..., default_flow_style=False, sort_keys=False)`

8. **`format_validation_errors()` helper function**:
   - Takes `ValidationError`, produces user-friendly string with YAML paths
   - Format integer indices with brackets: `items[0].value`
   - Include context if available

9. **Keep old class names as aliases** for import compatibility during transition. At the bottom of config.py, add backward-compatible aliases:
   ```python
   # Backward-compatible aliases (deprecated, will be removed in v0.3)
   ColorNormConfig = PreprocessingConfig  # Partial — users should migrate
   FrameSamplingConfig = PreprocessingConfig
   FeatureExtractionConfig = SparseMatchingConfig
   PairSelectionConfig = SparseMatchingConfig
   MatchingConfig = SparseMatchingConfig
   DenseStereoConfig = ReconstructionConfig
   FusionConfig = ReconstructionConfig
   SurfaceConfig = ReconstructionConfig
   OutlierRemovalConfig = ReconstructionConfig
   DeviceConfig = RuntimeConfig
   OutputConfig = RuntimeConfig
   VizConfig = RuntimeConfig
   BenchmarkConfig = RuntimeConfig
   EvaluationConfig = RuntimeConfig
   ```
   These aliases let `from aquamvs.config import DenseStereoConfig` still work. They point to the parent group class. This is NOT perfect (old `DenseStereoConfig(num_depths=256)` won't work since the fields are now in ReconstructionConfig), but it prevents import errors in downstream code.

10. **Update `src/aquamvs/__init__.py`**:
    - Export new config classes: `PreprocessingConfig`, `SparseMatchingConfig`, `DenseMatchingConfig`, `ReconstructionConfig`, `RuntimeConfig`, `PipelineConfig`
    - Remove old individual config class exports that no longer exist as separate classes
    - Add to `__all__`

11. **Preserve VALID_* constants** at module level for use by validators and external code:
    - `VALID_COLOR_NORM_METHODS`, `VALID_VIZ_STAGES`, `VALID_EXTRACTORS`, `VALID_MATCHERS`
  </action>
  <verify>
    - `pip install -e ".[dev]"` succeeds (new deps install)
    - `python -c "from aquamvs.config import PipelineConfig; p = PipelineConfig(); print(p.model_dump())"` works
    - `python -c "from aquamvs.config import PipelineConfig; p = PipelineConfig(); print(type(p.preprocessing))"` shows PreprocessingConfig
    - `python -c "from pydantic import ValidationError; from aquamvs.config import PipelineConfig; PipelineConfig(matcher_type='invalid')"` raises ValidationError
    - `python -c "from aquamvs.config import DenseStereoConfig"` still works (alias)
  </verify>
  <done>
    - config.py contains ~6 Pydantic BaseModel classes grouped by pipeline stage
    - Literal types enforce enum validation (matcher_type, pipeline_mode, etc.)
    - from_yaml() loads YAML with backward-compatible remapping of old field names
    - from_yaml() logs INFO for applied defaults on missing sections
    - Extra fields produce warnings, not errors
    - Validation collects all errors and formats with YAML paths
    - Old class name imports still work via aliases
    - pyproject.toml includes pydantic>=2.12.0 and tqdm>=4.66.0
  </done>
</task>

<task type="auto">
  <name>Task 2: Rewrite config tests for Pydantic models</name>
  <files>
    tests/test_config.py
  </files>
  <action>
1. **Rewrite `tests/test_config.py`** to test the new Pydantic config structure. The test file should cover:

   **TestPreprocessingConfig:**
   - Test defaults: `color_norm_enabled=False`, `color_norm_method="gain"`, `frame_start=0`, `frame_stop=None`, `frame_step=1`
   - Test custom values
   - Test invalid color_norm_method raises ValidationError (Literal type catches it)

   **TestSparseMatchingConfig:**
   - Test defaults for all fields (extractor_type, max_keypoints, detection_threshold, clahe_*, num_neighbors, include_center, filter_threshold)
   - Test custom values

   **TestDenseMatchingConfig:**
   - Test defaults (certainty_threshold=0.5, max_correspondences=100000)
   - Test custom values

   **TestReconstructionConfig:**
   - Test defaults for all fields
   - Test invalid window_size (even) raises ValidationError
   - Test invalid window_size (negative) raises ValidationError
   - Test invalid surface_method raises ValidationError (Literal type)
   - Test invalid cost_function raises ValidationError (Literal type)

   **TestRuntimeConfig:**
   - Test defaults for all fields
   - Test invalid device raises ValidationError
   - Test invalid viz_stages raises ValidationError
   - Test invalid benchmark_extractors raises ValidationError
   - Test quiet default is False

   **TestPipelineConfig:**
   - Test defaults: calibration_path="", output_dir="", pipeline_mode="full", matcher_type="lightglue"
   - Test invalid matcher_type raises ValidationError
   - Test invalid pipeline_mode raises ValidationError
   - Test all sub-configs have correct defaults

   **TestValidationErrorCollection:**
   - Create config with MULTIPLE invalid values (e.g., invalid matcher_type + invalid pipeline_mode)
   - Verify ValidationError contains multiple errors (not just the first one)
   - Verify error messages contain YAML-style paths

   **TestExtraFieldWarning:**
   - Create config with unknown keys
   - Verify warning is logged (use `caplog` fixture)
   - Verify config still loads successfully (not an error)

   **TestYAMLRoundTrip:**
   - Test round-trip with full config (save -> load -> compare)
   - Test round-trip with defaults only
   - Test partial YAML merges over defaults
   - Test empty YAML loads as defaults
   - Test None values serialize/deserialize correctly

   **TestBackwardCompatibility:**
   - Test loading YAML with OLD flat structure (top-level `color_norm`, `frame_sampling`, `feature_extraction`, etc.) still works
   - Verify values are remapped to new nested locations
   - Test loading YAML with NEW nested structure works
   - Test that old class name imports still resolve (DenseStereoConfig, etc.)

   **TestDefaultLogging:**
   - Use `caplog` fixture to capture log messages
   - Load a minimal config (just paths + matcher_type)
   - Verify INFO messages about applied defaults are logged

   **TestImports:**
   - Test that new config classes can be imported from `aquamvs.config`
   - Test that new config classes can be imported from `aquamvs` package
   - Test that old aliases still import

2. **Key testing patterns:**
   - Use `pytest.raises(ValidationError)` for Pydantic validation errors (not ValueError — Pydantic raises its own)
   - Use `caplog` fixture for log capture tests
   - Use `tmp_path` fixture instead of manual tempfile management
   - Use Pydantic's `model_validate()` for dict-based construction tests
  </action>
  <verify>
    - `pytest tests/test_config.py -v` passes all tests
    - `pytest tests/test_config.py -v --tb=short` shows no failures
  </verify>
  <done>
    - All new Pydantic config classes have test coverage for defaults and custom values
    - Validation error collection tested (multiple errors in one exception)
    - YAML round-trip tested for new structure
    - Backward compatibility tested (old YAML format loads correctly)
    - Extra field warnings tested with caplog
    - Default logging tested with caplog
    - Old import aliases tested
  </done>
</task>

</tasks>

<verification>
1. `pip install -e ".[dev]"` succeeds with pydantic and tqdm installed
2. `python -c "from aquamvs.config import PipelineConfig; PipelineConfig()"` creates valid default config
3. `python -c "from aquamvs.config import PipelineConfig; PipelineConfig(matcher_type='invalid')"` raises ValidationError with clear message
4. `pytest tests/test_config.py -v` all tests pass
5. Old YAML configs load without error via backward-compatible remapping
</verification>

<success_criteria>
- 14 dataclasses consolidated into ~6 Pydantic BaseModel classes grouped by pipeline stage
- Pydantic validation collects ALL errors before reporting (not fail-on-first)
- Error messages include YAML paths (e.g., reconstruction.num_depths)
- Unknown YAML keys produce warning, not error
- Minimal config (paths + matcher_type) loads with defaults filling the rest
- Missing optional sections logged at INFO level
- Backward-compatible YAML loading for existing config files
- All config tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-configuration-and-api-cleanup/02-01-SUMMARY.md`
</output>
