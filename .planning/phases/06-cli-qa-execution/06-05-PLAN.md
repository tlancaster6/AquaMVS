---
phase: 06-cli-qa-execution
plan: 05
type: execute
wave: 5
depends_on: ["06-02"]
files_modified:
  - .planning/qa/issues-found.md
autonomous: false

must_haves:
  truths:
    - "aquamvs profile produces a per-stage performance breakdown report"
    - "Pipeline(config).run() programmatic API completes without errors"
    - "All 7 CLI commands and Pipeline API have been QA-validated with real data"
  artifacts:
    - path: "profiling output"
      provides: "Per-stage timing report identifying performance bottlenecks"
  key_links:
    - from: "aquamvs profile"
      to: "profiling report"
      via: "PipelineProfiler wrapping pipeline execution with torch.profiler"
    - from: "Pipeline(config).run()"
      to: "reconstruction output"
      via: "Programmatic API equivalent to aquamvs run"
---

<objective>
QA the `aquamvs profile` command and the programmatic `Pipeline(config).run()` API. The profiler validates performance measurement tooling, and the Pipeline API smoke test confirms the library can be used programmatically (not just via CLI).

Purpose: Complete the QA pass by validating the final two interfaces — profiler CLI and programmatic API.
Output: Profiling report and confirmation that Pipeline API works.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-cli-qa-execution/06-02-SUMMARY.md
@src/aquamvs/cli.py
@src/aquamvs/profiling/__init__.py
@src/aquamvs/pipeline/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run profiler and Pipeline API smoke test</name>
  <files>.planning/qa/issues-found.md</files>
  <action>
Test the profiler CLI command and programmatic Pipeline API.

**Step 1: Run profiler**
- Run: `aquamvs profile config.yaml --frame 0`
  - This profiles a single frame through the full pipeline
  - Uses real data from config.yaml (same data as Plan 02 LightGlue run)
- If profile_pipeline raises NotImplementedError (stub from Phase 5), that's a known issue — log it
- If it succeeds, review the per-stage timing breakdown:
  - Are all stages present (undistortion, matching, depth_estimation, fusion, surface)?
  - Do timings seem plausible? (depth_estimation should be the bottleneck)
  - Is memory reporting present?

**Step 2: Test --output-dir flag (if profiler works)**
- Run: `aquamvs profile config.yaml --frame 0 --output-dir ./profiling`
- Check if Chrome trace JSON is produced (may be placeholder — log if not implemented)

**Step 3: Pipeline API smoke test**
- Run a minimal Python script to test programmatic API:
```python
python -c "
from aquamvs import Pipeline
from aquamvs.config import PipelineConfig

config = PipelineConfig.from_yaml('config.yaml')
config.runtime.device = 'cuda'
pipeline = Pipeline(config)
pipeline.run()
print('Pipeline API: OK')
"
```
- This should produce the same output as `aquamvs run config.yaml`
- If it fails with import errors or API issues, diagnose and fix

**Step 4: Final issue summary**
- Review .planning/qa/issues-found.md
- Add a final summary section listing all issues found across Plans 01-05
- Categorize as: fixed inline, logged for later, or won't fix
  </action>
  <verify>
- `aquamvs profile` either produces a timing report OR raises a clear NotImplementedError (both are acceptable outcomes)
- Pipeline(config).run() completes without errors
- .planning/qa/issues-found.md contains final issue summary
  </verify>
  <done>Profiler CLI tested, Pipeline API confirmed working, and all QA issues documented</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: User reviews profiler output and QA summary</name>
  <action>
Present profiler results and final QA summary for user review:
1. `aquamvs profile` executed — either produced a performance report or identified a stub that needs completion
2. `Pipeline(config).run()` programmatic API tested — smoke test passed
3. Final QA issue summary compiled in .planning/qa/issues-found.md

User should verify:
1. Review profiler output — if timing report was produced, check that stage breakdown makes sense
2. Confirm Pipeline API produced same output as CLI `run` command
3. Review .planning/qa/issues-found.md — are there any critical issues that need addressing before v1.0?
4. **Final QA assessment:** All 7 CLI commands tested, both matcher pathways validated, mesh export working, profiler tested, Pipeline API confirmed. Is the QA pass complete?

Resume signal: Type "approved" if QA pass is complete, or identify remaining issues that need resolution.
  </action>
  <verify>User confirms all CLI commands and Pipeline API have been validated</verify>
  <done>User has reviewed profiler output and confirmed Phase 6 QA pass is complete</done>
</task>

</tasks>

<verification>
- Profiler CLI runs (success or documented limitation)
- Pipeline API works programmatically
- All 7 CLI commands have been tested: preprocess, init, export-refs, run, benchmark, export-mesh, profile
- Pipeline(config).run() API also tested
- QA issues documented and categorized
</verification>

<success_criteria>
- Profiler produces timing breakdown or limitation is documented
- Pipeline(config).run() is a working programmatic API
- All QA issues logged with disposition (fixed/deferred/won't-fix)
- User confirms Phase 6 QA pass is complete
</success_criteria>

<output>
After completion, create `.planning/phases/06-cli-qa-execution/06-05-SUMMARY.md`
</output>
