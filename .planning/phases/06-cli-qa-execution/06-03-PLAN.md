---
phase: 06-cli-qa-execution
plan: 03
type: execute
wave: 3
depends_on: ["06-02"]
files_modified:
  - .planning/qa/issues-found.md
autonomous: false

must_haves:
  truths:
    - "aquamvs run completes a full RoMa+full reconstruction on real data with CUDA"
    - "aquamvs benchmark --compare produces a comparison table between LightGlue and RoMa runs"
    - "Both matcher pathways produce valid reconstruction outputs"
  artifacts:
    - path: "output_roma/frame_*/mesh_*.ply"
      provides: "RoMa reconstruction mesh for comparison"
    - path: "benchmark comparison output"
      provides: "Side-by-side metrics comparing LightGlue vs RoMa"
  key_links:
    - from: "config_roma.yaml"
      to: "aquamvs run"
      via: "Same pipeline with matcher_type: roma"
    - from: "aquamvs benchmark --compare"
      to: "run directories"
      via: "Loads metrics from both output directories"
---

<objective>
QA the RoMa+full execution path and the benchmark comparison command. Run reconstruction with RoMa matcher, then compare results against the LightGlue run from Plan 02.

Purpose: Validate the alternative matcher pathway and the comparison tooling work end-to-end.
Output: RoMa reconstruction outputs and a comparison report between the two matchers.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-cli-qa-execution/06-02-SUMMARY.md
@src/aquamvs/cli.py
@src/aquamvs/benchmark/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run RoMa+full reconstruction and benchmark comparison</name>
  <files>.planning/qa/issues-found.md</files>
  <action>
Execute RoMa+full reconstruction, then run benchmark comparison against LightGlue results.

**Step 1: Prepare RoMa config**
- Copy config.yaml to config_roma.yaml
- Edit config_roma.yaml:
  - Set `sparse_matching.matcher_type: roma` (or `matching.matcher_type: roma` — check actual field name in config.py)
  - Set `output_dir` to a different directory (e.g., ./output_roma) to avoid overwriting LightGlue results
  - Keep same mask_dir, calibration_path, device settings

**Step 2: Run RoMa pipeline**
- Run: `aquamvs run config_roma.yaml --device cuda`
- RoMa is slower and more VRAM-hungry than LightGlue — monitor GPU memory
- If CUDA OOM: reduce image resolution or use FAST preset
- Fix any blockers immediately

**Step 3: Verify RoMa outputs**
- Same verification as Plan 02: check depth maps, sparse cloud, fused cloud, mesh exist and are non-empty
- Quick point count comparison: RoMa typically produces denser matches than LightGlue

**Step 4: Run benchmark comparison**
- Run: `aquamvs benchmark config.yaml --compare ./output ./output_roma`
  - Note: benchmark command requires a config file as first positional arg plus --compare flag
  - If this syntax doesn't work, check the actual CLI signature and adjust
- Review comparison output: should show side-by-side metrics (point counts, mesh stats, runtime)
- If --compare fails, diagnose and fix — this may need the benchmark config format, not pipeline config

**Step 5: Log issues**
- Append any non-blocking issues to .planning/qa/issues-found.md
  </action>
  <verify>
- `aquamvs run config_roma.yaml` exits with code 0
- `ls output_roma/frame_000000/*.ply` shows point cloud and mesh files
- Benchmark comparison command produces readable output (table or text)
  </verify>
  <done>RoMa+full pipeline completes on real data, and benchmark comparison between LightGlue and RoMa produces a meaningful comparison report</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: User reviews RoMa output and comparison</name>
  <action>
Present RoMa results and benchmark comparison for user review:
1. RoMa+full reconstruction completed on same real data as LightGlue run
2. Benchmark comparison generated between the two matcher pathways

User should verify:
1. Open RoMa mesh (output_roma/frame_000000/mesh_*.ply) — compare surface quality to LightGlue mesh
2. Open RoMa depth maps — compare smoothness and coverage to LightGlue depth maps
3. Review benchmark comparison output — do the metrics make sense? (RoMa typically denser but slower)
4. Sanity check: both matchers should reconstruct the same approximate surface shape

Resume signal: Type "approved" if both matcher pathways produce acceptable results and comparison is meaningful, or describe issues.
  </action>
  <verify>User confirms both matchers produce acceptable results and comparison is meaningful</verify>
  <done>User has reviewed RoMa output and LightGlue vs RoMa comparison</done>
</task>

</tasks>

<verification>
- Both LightGlue and RoMa pipelines complete on real data
- Benchmark comparison runs without errors
- Comparison metrics are reasonable (neither matcher produces garbage)
</verification>

<success_criteria>
- RoMa+full pipeline completes on CUDA with valid outputs
- Benchmark --compare shows meaningful differences between matchers
- Both matchers reconstruct plausible underwater surface
- User confirms both outputs are acceptable quality
</success_criteria>

<output>
After completion, create `.planning/phases/06-cli-qa-execution/06-03-SUMMARY.md`
</output>
