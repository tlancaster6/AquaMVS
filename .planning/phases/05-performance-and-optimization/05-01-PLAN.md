---
phase: 05-performance-and-optimization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/aquamvs/benchmark/config.py
  - src/aquamvs/benchmark/metrics.py
  - src/aquamvs/benchmark/synthetic.py
  - src/aquamvs/benchmark/__init__.py
  - tests/test_benchmark/test_metrics.py
  - tests/test_benchmark/test_synthetic.py
autonomous: true

must_haves:
  truths:
    - "BenchmarkConfig can be loaded from YAML with test toggles, dataset paths, and regression thresholds"
    - "Accuracy metrics (completeness, geometric error) can be computed from reconstructed points vs ground truth"
    - "Synthetic flat plane and undulating surface scenes can be generated matching 12-camera ring geometry"
  artifacts:
    - path: "src/aquamvs/benchmark/config.py"
      provides: "BenchmarkConfig, BenchmarkDataset, BenchmarkTests Pydantic models"
      exports: ["BenchmarkConfig", "BenchmarkDataset", "BenchmarkTests"]
    - path: "src/aquamvs/benchmark/metrics.py"
      provides: "Accuracy metrics: completeness, geometric error, with tolerance-based accurate coverage"
      exports: ["compute_accuracy_metrics"]
    - path: "src/aquamvs/benchmark/synthetic.py"
      provides: "Synthetic scene generation: flat plane + undulating surface with analytic ground truth"
      exports: ["create_flat_plane_scene", "create_undulating_scene"]
  key_links:
    - from: "src/aquamvs/benchmark/config.py"
      to: "pydantic BaseModel"
      via: "Pydantic model validation with YAML loading"
      pattern: "class BenchmarkConfig.*BaseModel"
    - from: "src/aquamvs/benchmark/metrics.py"
      to: "open3d"
      via: "Point cloud distance computation"
      pattern: "compute_point_cloud_distance|compute_accuracy_metrics"
    - from: "src/aquamvs/benchmark/synthetic.py"
      to: "src/aquamvs/projection"
      via: "Project analytic surface into camera views for ground truth depth"
      pattern: "cast_ray|project"
---

<objective>
Create the benchmark foundation: configuration models, accuracy metric functions, and synthetic ground truth scene generation.

Purpose: All benchmark tests depend on config-driven test toggling, standardized accuracy metrics, and ground truth data. This plan establishes those shared foundations before the runner connects them.

Output: BenchmarkConfig Pydantic model with YAML I/O, compute_accuracy_metrics function, and synthetic scene generators for flat plane + undulating surface.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-performance-and-optimization/05-RESEARCH.md
@src/aquamvs/config.py
@src/aquamvs/benchmark/metrics.py
@src/aquamvs/surface.py
@src/aquamvs/projection/protocol.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Benchmark config models and accuracy metrics</name>
  <files>
    src/aquamvs/benchmark/config.py
    src/aquamvs/benchmark/metrics.py
    src/aquamvs/benchmark/__init__.py
    tests/test_benchmark/test_metrics.py
  </files>
  <action>
    Create `src/aquamvs/benchmark/config.py` with Pydantic models per user decision:

    - `BenchmarkDataset`: name (str), type (Literal["charuco", "synthetic_plane", "synthetic_surface"]), path (str), ground_truth_tolerance_mm (float | None = None)
    - `BenchmarkTests`: clahe_comparison (bool = True), execution_mode_comparison (bool = True), surface_reconstruction_comparison (bool = True) -- each benchmark test toggled in config YAML per user decision
    - `RegressionThresholds`: dict-like model with per-metric threshold percentages. Default 5% for accuracy metrics, 10% for runtime metrics per research recommendation
    - `BenchmarkConfig`: output_dir (str), datasets (list[BenchmarkDataset]), tests (BenchmarkTests), regression_thresholds (dict[str, float]), frames (int = 1), lightglue_extractor (Literal["superpoint", "aliked", "disk"] = "superpoint") -- user-selectable LightGlue extractor per decision
    - Add `from_yaml` classmethod and `to_yaml` method matching PipelineConfig pattern

    Rewrite `src/aquamvs/benchmark/metrics.py` to replace the existing sparse-only metrics with accuracy metrics per user decision:
    - `compute_accuracy_metrics(reconstructed_points: np.ndarray, ground_truth_mesh: o3d.geometry.TriangleMesh, tolerance_mm: float | None = None) -> dict[str, float]`
    - Return: mean_error_mm, median_error_mm, std_error_mm, raw_completeness_pct
    - If tolerance_mm provided: also return accurate_completeness_pct (fraction within tolerance)
    - For real data (ChArUco): gracefully skip tolerance-based completeness when dense ground truth unavailable per user decision
    - `compute_charuco_metrics(detected_corners: np.ndarray, projected_corners: np.ndarray) -> dict[str, float]` -- point-level error at detected corner positions
    - `compute_plane_fit_metrics(points: np.ndarray) -> dict[str, float]` -- plane fitting for overall shape/scale accuracy (ChArUco evaluation per user decision)

    Update `src/aquamvs/benchmark/__init__.py` to export new public API: BenchmarkConfig, compute_accuracy_metrics, compute_charuco_metrics, compute_plane_fit_metrics.

    Create `tests/test_benchmark/test_metrics.py` testing:
    - compute_accuracy_metrics with known point cloud vs known mesh (perfect match = 0 error, offset = measurable error)
    - tolerance-based completeness (some points within, some outside)
    - Graceful handling when tolerance_mm is None
    - compute_plane_fit_metrics with synthetic planar points + noise
  </action>
  <verify>
    Run `pytest tests/test_benchmark/test_metrics.py -v` -- all tests pass.
    Run `python -c "from aquamvs.benchmark import BenchmarkConfig; print(BenchmarkConfig.__fields__.keys())"` -- prints expected fields.
  </verify>
  <done>
    BenchmarkConfig loads from YAML with test toggles and dataset paths. Accuracy metrics compute completeness and geometric error with optional tolerance. Plane fit and ChArUco corner metrics available. All metric tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Synthetic scene generation</name>
  <files>
    src/aquamvs/benchmark/synthetic.py
    tests/test_benchmark/test_synthetic.py
  </files>
  <action>
    Create `src/aquamvs/benchmark/synthetic.py` with two scene generators per user decision:

    1. `create_flat_plane_scene(depth_z: float, bounds: tuple[float, float, float, float], resolution: float = 0.005) -> tuple[o3d.geometry.TriangleMesh, Callable[[np.ndarray, np.ndarray], np.ndarray]]`
       - Creates a flat plane mesh at given world Z depth (underwater target)
       - Returns mesh + analytic depth function that takes (x, y) -> z (trivially constant)
       - Default geometry: bounds matching camera ring FOV at water_z ~0.978m, target at ~1.1m

    2. `create_undulating_scene(base_depth_z: float, amplitude: float, wavelength: float, bounds: tuple[float, float, float, float], resolution: float = 0.005) -> tuple[o3d.geometry.TriangleMesh, Callable[[np.ndarray, np.ndarray], np.ndarray]]`
       - Creates an undulating sand-like surface with known analytic form per user decision
       - Surface: z(x,y) = base_depth_z + amplitude * (sin(2*pi*x/wavelength) + sin(2*pi*y/(wavelength*1.3)))
       - Returns mesh + analytic depth function for ground truth comparison
       - Default: amplitude ~5mm, wavelength ~0.05m (subtle undulation matching tank scale)

    3. `generate_ground_truth_depth_maps(scene_mesh: o3d.geometry.TriangleMesh, projection_models: dict[str, ProjectionModel], image_shape: tuple[int, int]) -> dict[str, np.ndarray]`
       - For each camera, ray-cast the mesh to produce ground truth depth maps
       - Use Open3D ray casting (RaycastingScene) -- do NOT hand-roll ray-mesh intersection
       - Convert hit distances to ray-depth parameterization matching pipeline convention
       - Invalid pixels (no hit) = NaN

    4. `get_reference_geometry() -> dict` -- returns the 12-camera ring geometry constants (0.635m radius, water_z ~0.978m, n_water = 1.333) matching experimental setup per user decision. Used by tests to create consistent synthetic scenes.

    Create `tests/test_benchmark/test_synthetic.py` testing:
    - Flat plane scene has correct Z values at sampled points
    - Undulating scene analytic function matches mesh vertices
    - Ground truth depth maps have expected shape and valid depth range
    - Reference geometry returns expected constants
  </action>
  <verify>
    Run `pytest tests/test_benchmark/test_synthetic.py -v` -- all tests pass.
    Run `python -c "from aquamvs.benchmark.synthetic import create_flat_plane_scene; mesh, fn = create_flat_plane_scene(1.1, (-0.2, 0.2, -0.2, 0.2)); print(f'Vertices: {len(mesh.vertices)}')"` -- prints vertex count > 0.
  </verify>
  <done>
    Flat plane and undulating surface scenes generate Open3D meshes with known analytic ground truth. Ground truth depth map generation via ray casting works for arbitrary projection models. Reference geometry constants match experimental 12-camera ring setup.
  </done>
</task>

</tasks>

<verification>
- `pytest tests/test_benchmark/ -v` passes all new tests
- BenchmarkConfig round-trips through YAML (load + save + reload matches)
- Synthetic scenes produce valid meshes with correct analytic properties
- Accuracy metrics produce correct values for known inputs
</verification>

<success_criteria>
- BenchmarkConfig Pydantic model validates from YAML with test toggles, dataset paths, regression thresholds, and frame count
- compute_accuracy_metrics returns mean/median/std error in mm plus completeness percentages
- Synthetic flat plane and undulating surface generators produce Open3D meshes matching experimental geometry
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-performance-and-optimization/05-01-SUMMARY.md`
</output>
