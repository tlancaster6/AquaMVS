---
phase: 05-performance-and-optimization
plan: 07
type: execute
wave: 1
depends_on: []
files_modified:
  - src/aquamvs/profiling/profiler.py
  - src/aquamvs/benchmark/synthetic.py
  - src/aquamvs/benchmark/datasets.py
  - src/aquamvs/benchmark/runner.py
  - src/aquamvs/pipeline/stages/undistortion.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "profile_pipeline() runs without NotImplementedError and returns a ProfileReport"
    - "Synthetic scene functions match the signatures called by datasets.py loaders"
    - "Benchmark runner computes actual accuracy metrics from reconstructed output instead of returning placeholder zeros"
  artifacts:
    - path: "src/aquamvs/profiling/profiler.py"
      provides: "Working profile_pipeline function integrated with Pipeline class"
      contains: "build_pipeline_context"
    - path: "src/aquamvs/benchmark/datasets.py"
      provides: "Synthetic data loaders with correct function signatures"
    - path: "src/aquamvs/benchmark/runner.py"
      provides: "Metric computation from reconstructed point clouds"
  key_links:
    - from: "src/aquamvs/profiling/profiler.py"
      to: "src/aquamvs/pipeline/runner.py"
      via: "process_frame call within PipelineProfiler context"
      pattern: "process_frame.*profiler"
    - from: "src/aquamvs/benchmark/datasets.py"
      to: "src/aquamvs/benchmark/synthetic.py"
      via: "matching function call signatures"
      pattern: "create_flat_plane_scene|create_undulating_scene"
---

<objective>
Fix broken wiring that prevents benchmark and profiling infrastructure from being executed.

Purpose: The phase 05 infrastructure was built but never run. Three categories of bugs prevent execution:
(1) profile_pipeline raises NotImplementedError instead of wrapping the pipeline,
(2) datasets.py calls synthetic scene functions with wrong argument signatures,
(3) runner.py returns placeholder zeros instead of computing metrics from output.
These must be fixed before plans 08 can run benchmarks and collect real measurements.

Output: All three subsystems (profiling, synthetic data, benchmark runner) are executable without errors.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-performance-and-optimization/05-VERIFICATION.md
@src/aquamvs/profiling/profiler.py
@src/aquamvs/profiling/analyzer.py
@src/aquamvs/benchmark/synthetic.py
@src/aquamvs/benchmark/datasets.py
@src/aquamvs/benchmark/runner.py
@src/aquamvs/benchmark/metrics.py
@src/aquamvs/pipeline/runner.py
@src/aquamvs/pipeline/stages/undistortion.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix profile_pipeline integration and undistortion instrumentation</name>
  <files>
    src/aquamvs/profiling/profiler.py
    src/aquamvs/pipeline/stages/undistortion.py
  </files>
  <action>
    **Fix profile_pipeline in profiler.py:**

    Replace the NotImplementedError stub with a working implementation that:
    1. Loads config and builds pipeline context via `build_pipeline_context(config)`
    2. Detects input type and opens video/image context manager (same pattern as `run_pipeline`)
    3. Creates a PipelineProfiler instance
    4. Within the profiler context, reads frame `frame` from the input source
    5. Calls `process_frame(frame, raw_images, ctx)` inside the profiler context
    6. Returns `profiler.get_report()`

    Import from `..pipeline.builder` (build_pipeline_context), `..pipeline.runner` (process_frame),
    `..io` (ImageDirectorySet, detect_input_type), and `aquacal.io.video` (VideoSet).

    Handle the case where CUDA sync is needed before and after profiling (check `torch.cuda.is_available()`).

    Also update the CLI's profile_command: remove the `except NotImplementedError` catch block since it should no longer be needed, but keep the general `except Exception` handler.

    **Add record_function to undistortion stage:**

    In `src/aquamvs/pipeline/stages/undistortion.py`:
    1. Add `from torch.profiler import record_function` to imports
    2. Wrap the body of `run_undistortion_stage` (after the docstring) in `with record_function("undistortion"):` to match the pattern used by all other stage files (sparse_matching, dense_matching, depth_estimation, fusion, surface)

    This ensures the profiler's analyzer can identify undistortion as a stage and measure its time/memory.
  </action>
  <verify>
    Run: `python -c "from aquamvs.profiling import profile_pipeline; print('import OK')"` — should not raise ImportError.
    Run: `python -c "from aquamvs.pipeline.stages.undistortion import run_undistortion_stage; import inspect; src = inspect.getsource(run_undistortion_stage); assert 'record_function' in src; print('instrumented OK')"` — should confirm instrumentation.
    Run: `python -c "from aquamvs.profiling.profiler import profile_pipeline; import inspect; src = inspect.getsource(profile_pipeline); assert 'NotImplementedError' not in src; print('stub removed OK')"` — should confirm stub is gone.
  </verify>
  <done>
    profile_pipeline is a real function that builds pipeline context, runs a single frame within PipelineProfiler, and returns ProfileReport. Undistortion stage is instrumented with record_function like all other stages.
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix synthetic data loaders and benchmark runner metric computation</name>
  <files>
    src/aquamvs/benchmark/synthetic.py
    src/aquamvs/benchmark/datasets.py
    src/aquamvs/benchmark/runner.py
  </files>
  <action>
    **Fix datasets.py synthetic loader signatures to match synthetic.py:**

    In `_load_synthetic_plane` (line ~148-186):
    - `create_flat_plane_scene` expects `(depth_z, bounds, resolution)` but datasets.py calls it with
      `(water_z=..., depth_below_water=..., width=..., height=...)` — wrong kwargs entirely.
    - Fix: Compute `depth_z = geometry["water_z_m"] + 0.2` (200mm below surface).
      Compute bounds as `(-0.25, 0.25, -0.15, 0.15)` (500mm x 300mm centered).
      Call `create_flat_plane_scene(depth_z=depth_z, bounds=bounds, resolution=0.005)`.
    - Also fix the key names: `geometry["water_z_m"]` not `geometry["water_z"]`,
      `geometry["n_water"]` stays correct.
    - Remove the calls to `generate_ground_truth_depth_maps(mesh=mesh, cameras=..., water_z=..., n_water=...)`
      since that function's signature is `(scene_mesh, projection_models, image_shape)` and requires
      actual ProjectionModel instances which we don't have in the synthetic loader.
      Instead, set `ground_truth_depths=None` and rely on the mesh + analytic function for ground truth.
      Store the analytic function on the DatasetContext (add an `analytic_fn` field to the dataclass, defaulting to None).

    In `_load_synthetic_surface` (line ~189-229):
    - Same pattern: `create_undulating_scene` expects `(base_depth_z, amplitude, wavelength, bounds, resolution)`
      but datasets.py calls it with `(water_z=..., base_depth=..., amplitude=..., wavelength=..., width=..., height=..., resolution=...)`.
    - Fix: Compute `base_depth_z = geometry["water_z_m"] + 0.2`.
      Compute bounds as `(-0.25, 0.25, -0.15, 0.15)`.
      Call `create_undulating_scene(base_depth_z=base_depth_z, amplitude=0.005, wavelength=0.05, bounds=bounds, resolution=0.005)`.
    - Same key name fixes and same removal of generate_ground_truth_depth_maps call.

    **Fix runner.py metric computation:**

    In `_run_pipeline_config` (line ~277-370):
    - Replace the TODO placeholder blocks (lines 333-354) with actual metric computation:
    - After `pipeline.run()`, look for reconstructed point cloud files in `output_dir` using glob pattern
      `"**/fused_points.ply"` or `"**/sparse_cloud.ply"` (the pipeline outputs).
    - If a point cloud file is found AND `dataset_ctx.mesh is not None`:
      Load the point cloud with `o3d.io.read_point_cloud()`,
      extract points as numpy array,
      call `compute_accuracy_metrics(points, dataset_ctx.mesh, dataset_ctx.tolerance_mm)`.
      Merge returned metrics dict with timing.
    - If no point cloud found, log a warning and return timing-only metrics (not placeholders with 0.0).
    - Import `compute_accuracy_metrics` from `.metrics` at top of file.
    - Import `open3d as o3d` at top of file.

    **Update DatasetContext:**
    - Add `analytic_fn: Callable | None = None` field (import Callable from typing).
    - Update `__all__` if needed.
  </action>
  <verify>
    Run: `python -c "from aquamvs.benchmark.datasets import load_dataset; from aquamvs.benchmark.config import BenchmarkDataset; print('import OK')"` — should succeed.
    Run: `python -c "from aquamvs.benchmark.runner import _run_pipeline_config; import inspect; src = inspect.getsource(_run_pipeline_config); assert 'TODO' not in src; print('TODOs removed')"` — should confirm no TODOs remain.
    Run: `python -c "from aquamvs.benchmark.datasets import DatasetContext; import inspect; src = inspect.getsource(DatasetContext); assert 'analytic_fn' in src; print('analytic_fn field exists')"` — should confirm field added.
  </verify>
  <done>
    Synthetic data loaders call scene generation functions with correct signatures. Benchmark runner computes actual accuracy metrics from reconstructed point clouds instead of returning placeholder zeros. DatasetContext includes analytic_fn field for ground truth evaluation.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from aquamvs.profiling import profile_pipeline; import inspect; assert 'NotImplementedError' not in inspect.getsource(profile_pipeline)"` passes
2. `python -c "from aquamvs.benchmark.datasets import _load_synthetic_plane; import inspect; src = inspect.getsource(_load_synthetic_plane); assert 'water_z=' not in src or 'water_z_m' in src"` passes
3. `python -c "from aquamvs.benchmark.runner import _run_pipeline_config; import inspect; assert 'TODO' not in inspect.getsource(_run_pipeline_config)"` passes
4. All existing tests pass: `pytest tests/ -x --ignore=tests/benchmarks/ -q`
</verification>

<success_criteria>
- profile_pipeline() is a real implementation that wraps pipeline execution in PipelineProfiler
- Synthetic scene generation functions are called with correct argument signatures
- Benchmark runner computes metrics from actual pipeline output files
- Undistortion stage is instrumented with record_function for profiler visibility
- No NotImplementedError or TODO placeholders remain in profiling/benchmark code paths
</success_criteria>

<output>
After completion, create `.planning/phases/05-performance-and-optimization/05-07-SUMMARY.md`
</output>
