---
phase: 05-performance-and-optimization
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/aquamvs/profiling/__init__.py
  - src/aquamvs/profiling/profiler.py
  - src/aquamvs/profiling/analyzer.py
  - src/aquamvs/pipeline/stages/depth_estimation.py
  - src/aquamvs/pipeline/stages/dense_matching.py
  - src/aquamvs/pipeline/stages/sparse_matching.py
  - src/aquamvs/pipeline/stages/fusion.py
  - src/aquamvs/pipeline/stages/surface.py
autonomous: true

must_haves:
  truths:
    - "Runtime profiling identifies top 3 performance bottlenecks with specific CPU/GPU time and memory measurements"
    - "Each pipeline stage has timing and memory instrumentation that can be enabled via config"
    - "Profile results are exportable as Chrome trace and structured summary"
  artifacts:
    - path: "src/aquamvs/profiling/profiler.py"
      provides: "torch.profiler wrapper with memory tracking and CUDA warmup"
      exports: ["PipelineProfiler", "profile_pipeline"]
    - path: "src/aquamvs/profiling/analyzer.py"
      provides: "Profile result parsing, sorting, and reporting"
      exports: ["analyze_profile", "ProfileReport"]
  key_links:
    - from: "src/aquamvs/profiling/profiler.py"
      to: "torch.profiler"
      via: "Wraps torch.profiler with record_function per stage"
      pattern: "torch\\.profiler|record_function|ProfilerActivity"
    - from: "src/aquamvs/profiling/profiler.py"
      to: "src/aquamvs/pipeline/stages"
      via: "Instruments stage functions with record_function"
      pattern: "record_function|run_.*_stage"
---

<objective>
Create profiling infrastructure using torch.profiler to identify runtime and memory bottlenecks across all pipeline stages.

Purpose: BEN-02 requires identifying top 3 bottlenecks with measurements. This plan instruments the pipeline stages and provides structured profiling output. Can run in parallel with Plan 02 (benchmark runner) since it has no dependency on benchmark infrastructure.

Output: PipelineProfiler class, per-stage instrumentation, and ProfileReport with ranked bottleneck identification.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-performance-and-optimization/05-RESEARCH.md
@src/aquamvs/pipeline/stages/depth_estimation.py
@src/aquamvs/pipeline/stages/dense_matching.py
@src/aquamvs/pipeline/stages/sparse_matching.py
@src/aquamvs/pipeline/stages/fusion.py
@src/aquamvs/pipeline/stages/surface.py
@src/aquamvs/pipeline/runner.py
@src/aquamvs/dense/plane_sweep.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Profiling infrastructure</name>
  <files>
    src/aquamvs/profiling/__init__.py
    src/aquamvs/profiling/profiler.py
    src/aquamvs/profiling/analyzer.py
  </files>
  <action>
    Create `src/aquamvs/profiling/` package.

    Create `src/aquamvs/profiling/profiler.py`:
    - `PipelineProfiler` class wrapping torch.profiler:
      - Constructor: `__init__(self, activities: list[str] = ["cpu", "cuda"], profile_memory: bool = True, record_shapes: bool = True, output_dir: Path | None = None)`
      - Uses `ProfilerActivity.CPU` and `ProfilerActivity.CUDA` per research recommendation
      - `profile_memory=True` per research recommendation (GPU memory often the bottleneck)
      - `record_shapes=True` per research anti-pattern warning
      - CUDA warmup before profiling per research pitfall 1: run dummy tensor op + synchronize
      - Context manager interface: `__enter__` / `__exit__` wrapping `torch.profiler.profile`
      - `stage(name: str)` method returning `torch.profiler.record_function(name)` context manager for stage-level instrumentation
      - `export_chrome_trace(path: Path)` -- exports Chrome trace JSON for visualization
      - `get_report() -> ProfileReport` -- produces structured report via analyzer

    - `profile_pipeline(config: PipelineConfig, frame: int = 0) -> ProfileReport` convenience function:
      - Creates PipelineProfiler
      - Runs single-frame pipeline within profiler context
      - Each pipeline stage wrapped with `record_function` labels matching stage names: "undistortion", "sparse_matching", "dense_matching", "depth_estimation", "fusion", "surface_reconstruction"
      - Also adds sub-labels for hot functions: "build_cost_volume", "grid_sample_warp", "extract_depth"
      - Returns ProfileReport

    Create `src/aquamvs/profiling/analyzer.py`:
    - `ProfileReport` dataclass:
      - stages: dict[str, StageProfile] -- per-stage timing and memory
      - top_bottlenecks: list[tuple[str, float, float]] -- (name, time_ms, memory_mb) sorted by time
      - total_time_ms: float
      - total_memory_peak_mb: float
      - device: str
    - `StageProfile` dataclass: name, cpu_time_ms, cuda_time_ms, self_cpu_time_ms, self_cuda_time_ms, cpu_memory_mb, cuda_memory_mb
    - `analyze_profile(prof: torch.profiler.profile) -> ProfileReport`:
      - Uses `prof.key_averages()` to extract per-operation metrics
      - Groups operations by stage label (record_function names)
      - Identifies top 3 bottlenecks by total time (CPU + CUDA)
      - Reports both runtime and memory per stage per user decision
    - `format_report(report: ProfileReport) -> str`:
      - Formats report as ASCII table using tabulate
      - Includes: stage name, CPU time, CUDA time, peak memory
      - Highlights top 3 bottlenecks

    Create `src/aquamvs/profiling/__init__.py` with exports: PipelineProfiler, profile_pipeline, ProfileReport, analyze_profile.
  </action>
  <verify>
    Run `python -c "from aquamvs.profiling import PipelineProfiler, profile_pipeline, ProfileReport; print('OK')"` -- imports without error.
    Run `python -c "from torch.profiler import profile, ProfilerActivity; print('torch.profiler available')"` -- confirms torch.profiler works.
  </verify>
  <done>
    PipelineProfiler wraps torch.profiler with memory tracking, CUDA warmup, and stage-level instrumentation. ProfileReport identifies top 3 bottlenecks with CPU/CUDA time and memory measurements. Chrome trace export available.
  </done>
</task>

<task type="auto">
  <name>Task 2: Instrument pipeline stages with record_function labels</name>
  <files>
    src/aquamvs/pipeline/stages/depth_estimation.py
    src/aquamvs/pipeline/stages/dense_matching.py
    src/aquamvs/pipeline/stages/sparse_matching.py
    src/aquamvs/pipeline/stages/fusion.py
    src/aquamvs/pipeline/stages/surface.py
    src/aquamvs/dense/plane_sweep.py
  </files>
  <action>
    Add `torch.profiler.record_function` context managers to each pipeline stage's hot path. This is lightweight instrumentation (zero overhead when profiler is not active).

    In `src/aquamvs/pipeline/stages/depth_estimation.py`:
    - Wrap the main `run_depth_estimation_stage` body with `record_function("depth_estimation")`
    - Add `from torch.profiler import record_function` import

    In `src/aquamvs/pipeline/stages/dense_matching.py`:
    - Wrap main function body with `record_function("dense_matching")`

    In `src/aquamvs/pipeline/stages/sparse_matching.py`:
    - Wrap main function body with `record_function("sparse_matching")`

    In `src/aquamvs/pipeline/stages/fusion.py`:
    - Wrap main function body with `record_function("fusion")`

    In `src/aquamvs/pipeline/stages/surface.py`:
    - Wrap main function body with `record_function("surface_reconstruction")`

    In `src/aquamvs/dense/plane_sweep.py`:
    - Wrap `build_cost_volume` function body with `record_function("build_cost_volume")`
    - Wrap each `_warp_source_at_depth` call region with `record_function("grid_sample_warp")`
    - Wrap `extract_depth` function body with `record_function("extract_depth")`
    - Add `torch.no_grad()` context to `build_cost_volume` if not already present -- plane sweep doesn't need gradients per research pitfall 2

    Keep changes minimal: only add record_function wrappers and torch.no_grad(). Do NOT restructure the logic. The record_function annotations have zero cost when no profiler is active.
  </action>
  <verify>
    Run `pytest tests/test_dense/ -v` -- existing tests still pass (instrumentation is transparent).
    Run `pytest tests/test_pipeline.py -v` -- pipeline tests still pass.
  </verify>
  <done>
    All pipeline stages and plane sweep hot functions are annotated with record_function labels. build_cost_volume wrapped with torch.no_grad(). Existing tests pass unchanged.
  </done>
</task>

</tasks>

<verification>
- `pytest tests/test_dense/ tests/test_pipeline.py -v` -- all existing tests pass
- Profiler imports work and classes are constructable
- record_function labels are present in all stage modules
- plane_sweep.py build_cost_volume uses torch.no_grad()
</verification>

<success_criteria>
- PipelineProfiler wraps torch.profiler with memory tracking, CUDA warmup, stage labels
- ProfileReport identifies top 3 bottlenecks ranked by time with memory data
- All pipeline stages instrumented with record_function (zero overhead when inactive)
- build_cost_volume wrapped in torch.no_grad()
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-performance-and-optimization/05-03-SUMMARY.md`
</output>
