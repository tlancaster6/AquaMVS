---
phase: 05-performance-and-optimization
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/aquamvs/benchmark/runner.py
  - src/aquamvs/benchmark/datasets.py
  - src/aquamvs/benchmark/__init__.py
  - src/aquamvs/cli.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "aquamvs benchmark config.yaml runs all enabled benchmark tests and produces structured results"
    - "Each benchmark test (CLAHE, execution mode, surface reconstruction) runs with correct configurations"
    - "Results directory contains per-config accuracy and timing metrics in summary.json"
  artifacts:
    - path: "src/aquamvs/benchmark/runner.py"
      provides: "Benchmark orchestrator running enabled tests from config"
      exports: ["run_benchmarks"]
    - path: "src/aquamvs/benchmark/datasets.py"
      provides: "Ground truth dataset loaders for ChArUco and synthetic data"
      exports: ["load_dataset", "load_charuco_ground_truth"]
    - path: "src/aquamvs/cli.py"
      provides: "Replaced benchmark CLI command accepting benchmark config YAML"
      contains: "benchmark_command"
  key_links:
    - from: "src/aquamvs/benchmark/runner.py"
      to: "src/aquamvs/benchmark/config.py"
      via: "BenchmarkConfig drives which tests to run"
      pattern: "BenchmarkConfig|config\\.tests"
    - from: "src/aquamvs/benchmark/runner.py"
      to: "src/aquamvs/pipeline"
      via: "Pipeline execution for each benchmark configuration"
      pattern: "Pipeline|run_pipeline|process_frame"
    - from: "src/aquamvs/benchmark/runner.py"
      to: "src/aquamvs/benchmark/metrics.py"
      via: "Compute accuracy metrics on pipeline output"
      pattern: "compute_accuracy_metrics|compute_charuco_metrics"
    - from: "src/aquamvs/cli.py"
      to: "src/aquamvs/benchmark/runner.py"
      via: "CLI invokes benchmark runner"
      pattern: "run_benchmarks|benchmark_command"
---

<objective>
Build the benchmark runner that orchestrates all benchmark tests (CLAHE, execution mode, surface reconstruction) and replace the existing CLI command.

Purpose: This connects the config + metrics + synthetic foundations (Plan 01) to actual pipeline execution, producing structured results that downstream plans (visualization, comparison, CI) consume.

Output: Working `aquamvs benchmark config.yaml` command that runs enabled tests and writes structured results.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-performance-and-optimization/05-RESEARCH.md
@.planning/phases/05-performance-and-optimization/05-01-SUMMARY.md
@src/aquamvs/benchmark/runner.py
@src/aquamvs/benchmark/metrics.py
@src/aquamvs/cli.py
@src/aquamvs/pipeline/__init__.py
@src/aquamvs/pipeline/runner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Dataset loaders and benchmark runner</name>
  <files>
    src/aquamvs/benchmark/datasets.py
    src/aquamvs/benchmark/runner.py
    src/aquamvs/benchmark/__init__.py
    pyproject.toml
  </files>
  <action>
    Create `src/aquamvs/benchmark/datasets.py`:
    - `load_dataset(dataset: BenchmarkDataset) -> DatasetContext` -- loads ground truth based on dataset type
    - For "charuco": load calibration data + detect ChArUco corners on undistorted images. Use `cv2.aruco.detectMarkers` on undistorted images per research pitfall 5
    - For "synthetic_plane" / "synthetic_surface": call synthetic scene generators from Plan 01, generate ground truth depth maps
    - `DatasetContext` dataclass: mesh (o3d.geometry.TriangleMesh | None), ground_truth_depths (dict[str, np.ndarray] | None), charuco_corners (dict[str, np.ndarray] | None), tolerance_mm (float | None)

    Rewrite `src/aquamvs/benchmark/runner.py` completely (replacing existing sparse-only benchmark):
    - `run_benchmarks(config: BenchmarkConfig) -> BenchmarkRunResult` -- main orchestrator
    - Reads benchmark config, creates timestamped run directory under config.output_dir
    - Copies config YAML into run directory (config lives in same directory as output per user decision)
    - For each enabled test in config.tests:

    **CLAHE comparison** (if config.tests.clahe_comparison):
    - Test CLAHE on vs off across LightGlue with SuperPoint, ALIKED, DISK extractors, sparse mode per user decision
    - Also test with RoMa in sparse mode
    - For each config variant: modify PipelineConfig, run pipeline on each dataset, compute accuracy metrics, record timing
    - Use `time.perf_counter()` with `torch.cuda.synchronize()` for timing per research pitfall 6

    **Execution mode comparison** (if config.tests.execution_mode_comparison):
    - Test all 4 modes: LightGlue+sparse, LightGlue+full, RoMa+sparse, RoMa+full per user decision
    - LightGlue uses config.lightglue_extractor (user-selectable, default superpoint)
    - For each: run pipeline, compute accuracy + timing

    **Surface reconstruction comparison** (if config.tests.surface_reconstruction_comparison):
    - Test Poisson vs heightfield vs BPA per user decision
    - Use same depth maps (run depth estimation once), vary only surface method
    - Compute accuracy metrics for each method's mesh output

    Each test produces a `TestResult` with: test_name, per_config results (config_name -> {accuracy_metrics, timing_seconds, memory_peak_mb}), stored as JSON in results/{test_name}/

    `BenchmarkRunResult` dataclass: run_id (str), run_dir (Path), test_results (dict[str, TestResult]), summary (dict)

    Write `results/summary.json` at end with aggregated metrics across all tests.

    Add `tabulate` to pyproject.toml dependencies. Print ASCII summary table to terminal using tabulate per user decision (grid format, .1f for floats).

    Update `src/aquamvs/benchmark/__init__.py` to export run_benchmarks, BenchmarkRunResult.
  </action>
  <verify>
    Run `python -c "from aquamvs.benchmark import run_benchmarks; print('OK')"` -- imports without error.
    Verify tabulate is in pyproject.toml dependencies.
  </verify>
  <done>
    Benchmark runner orchestrates CLAHE, execution mode, and surface reconstruction tests from YAML config. Each test produces structured JSON results with accuracy metrics and timing. ASCII summary table printed to terminal.
  </done>
</task>

<task type="auto">
  <name>Task 2: CLI replacement and integration</name>
  <files>
    src/aquamvs/cli.py
  </files>
  <action>
    Replace the existing `benchmark_command` function in cli.py per user decision (no backward compatibility concern):

    Old signature: `benchmark_command(config_path: Path, frame: int = 0)`
    New signature: `benchmark_command(config_path: Path)` -- now takes a benchmark config YAML, not a pipeline config

    Update the argparse subparser for "benchmark":
    - Remove `--frame` argument (frame count now in benchmark config YAML)
    - config_path positional arg now points to benchmark config YAML
    - Add `--compare` optional argument: `--compare run1_dir run2_dir` for lightweight comparison (Plan 05 will implement the comparison logic, but wire the CLI arg now)
    - Add `--visualize` flag for plot generation per user decision (Plan 05 will implement, but add the flag now)

    The benchmark command should:
    1. Load BenchmarkConfig from YAML
    2. Call run_benchmarks(config)
    3. Print summary table
    4. If --compare provided, print "Comparison not yet implemented" (placeholder for Plan 05)
    5. If --visualize provided, print "Visualization not yet implemented" (placeholder for Plan 05)

    Update the main() argument parser to wire the new benchmark subcommand.
  </action>
  <verify>
    Run `aquamvs benchmark --help` -- shows updated help with config_path, --compare, --visualize.
    Run `python -c "from aquamvs.cli import benchmark_command; print('OK')"` -- imports without error.
  </verify>
  <done>
    `aquamvs benchmark config.yaml` replaces old benchmark command. Accepts benchmark config YAML. --compare and --visualize flags wired (placeholders for Plan 05). Old frame-based benchmark fully replaced.
  </done>
</task>

</tasks>

<verification>
- `aquamvs benchmark --help` shows updated CLI
- Benchmark runner imports cleanly and has correct test orchestration logic
- tabulate dependency added to pyproject.toml
- Results directory structure: {output_dir}/{run_id}/config.yaml, results/summary.json, results/{test_name}/
</verification>

<success_criteria>
- `aquamvs benchmark config.yaml` runs enabled benchmark tests from YAML config
- CLAHE, execution mode, and surface reconstruction tests produce per-config accuracy metrics and timing
- Results written to structured directory with summary.json
- ASCII summary table printed to terminal
- Old benchmark command fully replaced
</success_criteria>

<output>
After completion, create `.planning/phases/05-performance-and-optimization/05-02-SUMMARY.md`
</output>
