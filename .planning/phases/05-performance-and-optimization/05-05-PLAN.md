---
phase: 05-performance-and-optimization
plan: 05
type: execute
wave: 3
depends_on: ["05-02"]
files_modified:
  - src/aquamvs/benchmark/visualization.py
  - src/aquamvs/benchmark/comparison.py
  - src/aquamvs/benchmark/__init__.py
  - src/aquamvs/cli.py
autonomous: true

must_haves:
  truths:
    - "aquamvs benchmark --visualize generates error heatmaps, bar charts, and depth map comparisons"
    - "aquamvs benchmark --compare run1 run2 shows summary table with absolute and percent deltas"
    - "Regression detection alerts when metrics degrade beyond configured thresholds"
  artifacts:
    - path: "src/aquamvs/benchmark/visualization.py"
      provides: "Plot generation: error heatmaps, grouped bar charts, depth map comparisons"
      exports: ["generate_visualizations"]
    - path: "src/aquamvs/benchmark/comparison.py"
      provides: "Benchmark diff and regression detection"
      exports: ["compare_runs", "detect_regressions"]
  key_links:
    - from: "src/aquamvs/benchmark/visualization.py"
      to: "matplotlib"
      via: "Generates comparison plots saved to results subfolders"
      pattern: "plt\\.savefig|imshow|bar"
    - from: "src/aquamvs/benchmark/comparison.py"
      to: "src/aquamvs/benchmark/runner.py"
      via: "Loads summary.json from two runs for comparison"
      pattern: "summary\\.json|compare_runs"
    - from: "src/aquamvs/cli.py"
      to: "src/aquamvs/benchmark/comparison.py"
      via: "CLI --compare flag triggers comparison"
      pattern: "compare_runs|--compare"
---

<objective>
Implement benchmark visualization (plots) and run comparison with regression detection.

Purpose: Users need visual comparison of configs (error heatmaps, bar charts) and lightweight diff between runs to detect regressions. Completes the benchmark reporting per user decisions.

Output: --visualize generates plots, --compare produces diff table with regression alerts.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-performance-and-optimization/05-RESEARCH.md
@.planning/phases/05-performance-and-optimization/05-02-SUMMARY.md
@src/aquamvs/benchmark/visualization.py
@src/aquamvs/benchmark/runner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Visualization module</name>
  <files>
    src/aquamvs/benchmark/visualization.py
    src/aquamvs/benchmark/__init__.py
  </files>
  <action>
    Rewrite `src/aquamvs/benchmark/visualization.py` (replacing existing sparse-only visualization) per user decision on --visualize flag:

    `generate_visualizations(run_dir: Path, results: BenchmarkRunResult) -> list[Path]`:
    - Generates all visualization types, saves to {run_dir}/results/plots/
    - Returns list of generated plot file paths

    Plot types per user decision:

    1. **Error heatmaps** (`_plot_error_heatmaps`):
       - For each config that has spatial error data: matplotlib imshow with "hot" colormap
       - Spatial error map per config (if depth maps available)
       - Colorbar labeled "Error (mm)"
       - Save as PNG at 150 DPI per research code example

    2. **Grouped bar charts** (`_plot_accuracy_bars`, `_plot_timing_bars`):
       - Accuracy: grouped bars for mean_error_mm, completeness_pct across configs
       - Timing: grouped bars for per-stage timing across configs
       - Use tabulate-style config names as x-axis labels
       - Save as PNG

    3. **Depth map side-by-side comparisons** (`_plot_depth_comparisons`):
       - For each test with depth maps: side-by-side subplots showing depth maps from different configs
       - Common colorbar, same depth range
       - Annotate with config name and key metrics
       - Save as PNG

    Use matplotlib only (already a dependency). Follow research code example patterns for styling.

    Update `__init__.py` to export generate_visualizations.
  </action>
  <verify>
    Run `python -c "from aquamvs.benchmark.visualization import generate_visualizations; print('OK')"` -- imports without error.
  </verify>
  <done>
    --visualize flag generates error heatmaps, grouped bar charts for accuracy/timing, and depth map side-by-side comparisons. All plots saved to results/plots/ subfolder.
  </done>
</task>

<task type="auto">
  <name>Task 2: Comparison and regression detection</name>
  <files>
    src/aquamvs/benchmark/comparison.py
    src/aquamvs/benchmark/__init__.py
    src/aquamvs/cli.py
  </files>
  <action>
    Create `src/aquamvs/benchmark/comparison.py`:

    `compare_runs(run1_dir: Path, run2_dir: Path) -> ComparisonResult`:
    - Loads summary.json from both run directories
    - Per user decision: summary table with absolute and percent deltas for key metrics
    - `ComparisonResult` dataclass: run1_id, run2_id, metric_deltas (dict[str, MetricDelta]), regressions (list[str])
    - `MetricDelta` dataclass: metric_name, run1_value, run2_value, absolute_delta, percent_delta, is_regression

    `detect_regressions(baseline: dict[str, float], current: dict[str, float], thresholds: dict[str, float]) -> dict[str, tuple[float, float, bool]]`:
    - Per user decision: fixed percentage thresholds for regression detection
    - Per research recommendation: 5% default for accuracy metrics, 10% for runtime metrics
    - For error metrics (contains "error"): regression if current > baseline by threshold
    - For coverage/completeness metrics: regression if current < baseline by threshold
    - For timing metrics (contains "time"): regression if current > baseline by threshold
    - Follow research code example pattern exactly

    `format_comparison(result: ComparisonResult) -> str`:
    - ASCII table via tabulate showing metric, run1 value, run2 value, delta, % delta, status (OK / REGRESSION)
    - Highlight regressions with "REGRESSION" marker

    Wire into CLI (`src/aquamvs/cli.py`):
    - Replace the "--compare not yet implemented" placeholder from Plan 02
    - `--compare run1_dir run2_dir` calls compare_runs and prints formatted comparison
    - Replace "--visualize not yet implemented" placeholder: call generate_visualizations when flag is set

    Update `__init__.py` to export compare_runs, detect_regressions.
  </action>
  <verify>
    Run `python -c "from aquamvs.benchmark.comparison import compare_runs, detect_regressions; print('OK')"` -- imports without error.
    Run `aquamvs benchmark --help` -- shows --compare and --visualize without "not yet implemented" notes.
  </verify>
  <done>
    `aquamvs benchmark --compare run1 run2` produces diff table with absolute and percent deltas. Regression detection uses per-metric thresholds (5% accuracy, 10% runtime). --visualize generates all plots. CLI fully wired.
  </done>
</task>

</tasks>

<verification>
- Visualization module generates correct plot types (error heatmaps, bar charts, depth comparisons)
- Comparison loads two summary.json files and produces correct deltas
- Regression detection flags metrics exceeding thresholds
- CLI --compare and --visualize are functional (not placeholders)
</verification>

<success_criteria>
- --visualize generates 3 plot types saved to results/plots/
- --compare produces ASCII summary table with absolute and percent deltas
- Regression detection uses configurable per-metric thresholds (5% accuracy, 10% runtime default)
- All imports clean, CLI fully wired
</success_criteria>

<output>
After completion, create `.planning/phases/05-performance-and-optimization/05-05-SUMMARY.md`
</output>
