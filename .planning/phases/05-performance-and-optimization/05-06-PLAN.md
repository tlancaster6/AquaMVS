---
phase: 05-performance-and-optimization
plan: 06
type: execute
wave: 4
depends_on: ["05-04", "05-05"]
files_modified:
  - tests/benchmarks/test_ci_benchmarks.py
  - .github/workflows/ci.yml
  - .gitignore
  - docs/benchmarks.rst
  - src/aquamvs/profiling/profiler.py
  - src/aquamvs/cli.py
autonomous: true

must_haves:
  truths:
    - "CI runs synthetic-only benchmarks in under 1 minute"
    - "Results stored in .benchmarks/ directory (gitignored) with committed summary"
    - "Profiling CLI command produces structured report identifying top 3 bottlenecks"
    - "Sphinx docs page documents benchmark results from a published baseline"
  artifacts:
    - path: "tests/benchmarks/test_ci_benchmarks.py"
      provides: "Fast synthetic-only CI benchmarks under 1 minute"
      contains: "test_ci_"
    - path: ".gitignore"
      provides: ".benchmarks/ directory excluded from git"
      contains: ".benchmarks"
    - path: "docs/benchmarks.rst"
      provides: "Sphinx documentation page for benchmark results"
      contains: "Benchmark Results"
  key_links:
    - from: "tests/benchmarks/test_ci_benchmarks.py"
      to: "src/aquamvs/benchmark/synthetic.py"
      via: "Uses synthetic scenes for fast CI benchmarks"
      pattern: "create_flat_plane_scene|create_undulating_scene"
    - from: ".github/workflows/ci.yml"
      to: "tests/benchmarks/"
      via: "CI runs benchmark tests"
      pattern: "test_ci_benchmarks|benchmarks"
---

<objective>
Wire CI benchmarks, add profiling CLI command, create Sphinx docs page, and finalize integration.

Purpose: Completes the phase by connecting all components: CI regression detection (synthetic-only, under 1 minute per user decision), profiling command for users, documentation of results, and .benchmarks/ gitignore setup.

Output: CI benchmarks, `aquamvs profile` command, docs/benchmarks.rst, .benchmarks/ in .gitignore.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-performance-and-optimization/05-RESEARCH.md
@.planning/phases/05-performance-and-optimization/05-04-SUMMARY.md
@.planning/phases/05-performance-and-optimization/05-05-SUMMARY.md
@src/aquamvs/cli.py
@.github/workflows/ci.yml
@.gitignore
</context>

<tasks>

<task type="auto">
  <name>Task 1: CI benchmarks and .benchmarks setup</name>
  <files>
    tests/benchmarks/test_ci_benchmarks.py
    .github/workflows/ci.yml
    .gitignore
  </files>
  <action>
    Create `tests/benchmarks/__init__.py` (empty).

    Create `tests/benchmarks/test_ci_benchmarks.py` per user decision (CI runs synthetic-only, under 1 minute):
    - Uses pytest fixtures for synthetic scenes (flat plane only for speed -- CI subset selection per Claude's discretion)
    - Tests are focused on regression detection: verify metrics don't degrade below hardcoded thresholds

    Test functions (each must be fast -- target total suite under 60 seconds):
    1. `test_ci_synthetic_plane_lightglue_sparse()` -- LightGlue+sparse on flat plane, verify coverage > X%, error < Y mm
    2. `test_ci_synthetic_plane_roma_sparse()` -- RoMa+sparse on flat plane, same checks
    3. `test_ci_depth_extraction_correctness()` -- Unit test: feed known cost volume, verify depth extraction accuracy
    4. `test_ci_metric_computation()` -- Unit test: verify metric functions produce correct values on known inputs

    Use small image sizes (e.g., 64x64 or 128x128) and few depth hypotheses (16-32) for speed. The goal is regression detection, not accuracy measurement.

    Mark all tests with `@pytest.mark.benchmark` for selective running.
    Mark GPU tests with `@pytest.mark.skipif(not torch.cuda.is_available())`.

    Update `.github/workflows/ci.yml`:
    - Add benchmark test step: `pytest tests/benchmarks/ -m benchmark --timeout=120`
    - Run after main test suite
    - Allow failure (benchmarks are advisory, not blocking) -- use `continue-on-error: true`

    Update `.gitignore`:
    - Add `.benchmarks/` directory per user decision (results stored there, gitignored)
    - Add `*.prof` and `*.chrome_trace.json` for profiling artifacts
  </action>
  <verify>
    Run `pytest tests/benchmarks/ -v --timeout=120` -- all CI benchmark tests pass in under 60 seconds.
    Verify `.benchmarks/` is in .gitignore.
  </verify>
  <done>
    CI synthetic-only benchmarks run in under 1 minute. .benchmarks/ gitignored. CI workflow updated to run benchmarks as advisory step.
  </done>
</task>

<task type="auto">
  <name>Task 2: Profiling CLI command and Sphinx docs page</name>
  <files>
    src/aquamvs/cli.py
    src/aquamvs/profiling/profiler.py
    docs/benchmarks.rst
  </files>
  <action>
    Add `aquamvs profile` CLI command to `src/aquamvs/cli.py`:
    - `profile_command(config_path: Path, frame: int = 0, output_dir: Path | None = None)`
    - Loads PipelineConfig from YAML
    - Calls `profile_pipeline(config, frame)` from profiling module
    - Prints ProfileReport as ASCII table (top 3 bottlenecks highlighted)
    - If --output-dir provided, exports Chrome trace to {output_dir}/profile_trace.json
    - Add argparse subparser "profile" with: config_path (positional), --frame (default 0), --output-dir (optional)

    Update `profile_pipeline` in `src/aquamvs/profiling/profiler.py` if needed to support the CLI flow (ensure it works with just a config path and frame index).

    Create `docs/benchmarks.rst` per user decision (auto-generated Sphinx docs page from published baseline):
    - Title: "Benchmark Results"
    - Introduction explaining the benchmark suite (what it measures, how to run)
    - Section: "Running Benchmarks" with CLI usage examples
    - Section: "Interpreting Results" explaining metrics (completeness, geometric error)
    - Section: "Quality Presets" documenting fast/balanced/quality presets and their parameters
    - Section: "Profiling" with `aquamvs profile` usage
    - Section: "Baseline Results" with placeholder table (to be filled from first published baseline run per user decision -- intentionally updated, not automatic)
    - Section: "Regression Detection" explaining --compare and thresholds
    - Add to docs/index.rst toctree (if it exists)

    Note: The baseline results section is intentionally a placeholder per user decision. The user will publish baseline results later and update this page.
  </action>
  <verify>
    Run `aquamvs profile --help` -- shows profile subcommand with correct arguments.
    Run `python -c "import docutils.parsers.rst; print('rst parseable')"` or just verify docs/benchmarks.rst exists and has expected sections.
  </verify>
  <done>
    `aquamvs profile config.yaml` runs profiling and prints top 3 bottlenecks. docs/benchmarks.rst documents benchmark suite, quality presets, profiling, and has placeholder for baseline results. CI benchmarks wired and running.
  </done>
</task>

</tasks>

<verification>
- `pytest tests/benchmarks/ -v --timeout=120` passes in under 60 seconds
- `aquamvs profile --help` and `aquamvs benchmark --help` both work
- docs/benchmarks.rst has all required sections
- .benchmarks/ in .gitignore
- CI workflow includes benchmark step
</verification>

<success_criteria>
- CI synthetic-only benchmarks pass in under 1 minute (BEN-04 regression tracking)
- `aquamvs profile` CLI command identifies top 3 bottlenecks (BEN-02)
- docs/benchmarks.rst documents the full benchmark suite
- .benchmarks/ gitignored, CI benchmarks advisory (non-blocking)
- Phase 5 success criteria all met: BEN-01 (accuracy comparison via runner), BEN-02 (profiling), BEN-03 (optimization), benchmark suite (CI + local)
</success_criteria>

<output>
After completion, create `.planning/phases/05-performance-and-optimization/05-06-SUMMARY.md`
</output>
