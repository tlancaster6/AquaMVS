{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tlancaster6/AquaMVS/blob/main/docs/tutorial/notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# End-to-End Reconstruction Tutorial\n",
    "\n",
    "This tutorial walks through a complete multi-view stereo reconstruction workflow using the AquaMVS Python API. We start from synchronized multi-camera images, run the reconstruction pipeline, and produce a 3D surface mesh. This is approximately the pipeline that runs when you use the CLI command \"aquamvs run\". New users are encouraged to try the CLI before digging into the python API. \n",
    "\n",
    "By the end of this tutorial, you will have:\n",
    "- Loaded and inspected a pipeline configuration\n",
    "- Executed the reconstruction pipeline\n",
    "- Examined intermediate outputs (depth maps, consistency maps)\n",
    "- Visualized the fused point cloud\n",
    "- Exported the final mesh to various formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d84ee9-eff2-4b4f-9445-c6c5b2315fb6",
   "metadata": {},
   "source": [
    "## GPU Availability Check\n",
    "\n",
    "It is highly recommended that you run this notebook (and aquamvs in general) on a GPU-enabled machine or runtime if you want reasonable runtimes. The below cell will check for GPU availability. If you are running locally and it prints false, confirm that your computer has a CUDA-capable GPU and that you followed the pytorch install instructions in the Installation section of the docs. If you are on Colab, check you are using a GPU runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430adae7-5be4-4ba5-bfa9-69c68d6aa75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110064de-b369-42f9-a80d-960add1fa870",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install aquamvs (if not already installed), download the example dataset, and import key modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AquaMVS (run this cell in Colab; skip locally if already installed)\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if importlib.util.find_spec(\"aquamvs\") is None:\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"torch\",\n",
    "            \"torchvision\",\n",
    "            \"--index-url\",\n",
    "            \"https://download.pytorch.org/whl/cpu\",\n",
    "            \"-q\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"git+https://github.com/cvg/LightGlue.git@edb2b83\",\n",
    "            \"git+https://github.com/tlancaster6/RoMaV2.git\",\n",
    "            \"aquamvs\",\n",
    "            \"-q\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already present at aquamvs-example-dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_URL = \"https://zenodo.org/records/18702024/files/aquamvs-example-dataset.zip\"\n",
    "DATASET_DIR = Path(\"aquamvs-example-dataset\")\n",
    "\n",
    "if not DATASET_DIR.exists():\n",
    "    print(\"Downloading example dataset...\")\n",
    "    urllib.request.urlretrieve(DATASET_URL, \"aquamvs-example-dataset.zip\")\n",
    "    with zipfile.ZipFile(\"aquamvs-example-dataset.zip\") as zf:\n",
    "        zf.extractall(DATASET_DIR)\n",
    "    os.remove(\"aquamvs-example-dataset.zip\")\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(f\"Dataset already present at {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": "import logging\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom aquamvs import Pipeline, PipelineConfig\n\n# Enable logging so pipeline stages print progress\nlogging.basicConfig(level=logging.INFO, format=\"%(name)s - %(message)s\")\n\n# Change into the dataset directory so relative config paths resolve correctly\nos.chdir(DATASET_DIR)\nCONFIG_PATH = Path(\"config.yaml\")"
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Configuration\n",
    "\n",
    "The pipeline configuration defines all parameters for reconstruction: camera paths, calibration, feature matching settings, depth estimation parameters, and output options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aquamvs.config - Using default: preprocessing (all defaults)\n",
      "aquamvs.config - Using default: sparse_matching (all defaults)\n",
      "aquamvs.config - Using default: dense_matching (all defaults)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameras: ['e3v8250', 'e3v829d', 'e3v82e0', 'e3v82f9', 'e3v831e', 'e3v832e', 'e3v8334', 'e3v83e9', 'e3v83eb', 'e3v83ee', 'e3v83ef', 'e3v83f0', 'e3v83f1']\n",
      "Output directory: ./output\n",
      "Extractor type: superpoint\n",
      "Pipeline mode: full\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from YAML\n",
    "config = PipelineConfig.from_yaml(CONFIG_PATH)\n",
    "\n",
    "# Inspect key parameters\n",
    "print(f\"Cameras: {list(config.camera_input_map.keys())}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Extractor type: {config.sparse_matching.extractor_type}\")\n",
    "print(f\"Pipeline mode: {config.pipeline_mode}\")\n",
    "print(f\"Device: {config.runtime.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "**Expected output:** List of camera names (e.g., `['e3v82e0', 'e3v82e1', ...]`), output directory path, extractor type (`'superpoint'`), pipeline mode (`'full'` or `'sparse'`), and device (`'cpu'` or `'cuda'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "## 2. Run the Pipeline\n",
    "\n",
    "The `Pipeline` class provides the primary programmatic interface. Calling `.run()` executes the full reconstruction workflow:\n",
    "\n",
    "1. **Undistortion**: Apply camera calibration to remove lens distortion\n",
    "2. **Feature Matching**: Extract and match features across camera pairs (LightGlue or RoMa)\n",
    "3. **Triangulation**: Compute 3D points from feature correspondences (sparse mode) or...\n",
    "4. **Plane Sweep Stereo**: Dense depth estimation via photometric cost volume (full mode)\n",
    "5. **Depth Fusion**: Merge multi-view depth maps into a single point cloud\n",
    "6. **Surface Reconstruction**: Generate a triangle mesh from the point cloud\n",
    "\n",
    "This step can take a long time to run, depending on image resolution, number of cameras, and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aquamvs.pipeline.builder - Loading calibration from ./calibration.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Calibration file not found: calibration.json",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m pipeline = Pipeline(config)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Run reconstruction\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# This will process all frames according to config.preprocessing settings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\AquaMVS\\src\\aquamvs\\pipeline\\runner.py:263\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run the full reconstruction pipeline.\u001b[39;00m\n\u001b[32m    259\u001b[39m \n\u001b[32m    260\u001b[39m \u001b[33;03mEquivalent to calling run_pipeline(config).\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# Delegate to run_pipeline\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\AquaMVS\\src\\aquamvs\\pipeline\\runner.py:168\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run the full reconstruction pipeline over video frames.\u001b[39;00m\n\u001b[32m    152\u001b[39m \n\u001b[32m    153\u001b[39m \u001b[33;03mUses a two-pass architecture to avoid Open3D OpenGL / CUDA GPU memory\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m \u001b[33;03m    config: Full pipeline configuration.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# One-time setup\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m ctx = \u001b[43mbuild_pipeline_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# --- Compute pass ---\u001b[39;00m\n\u001b[32m    171\u001b[39m input_type = detect_input_type(config.camera_input_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\AquaMVS\\src\\aquamvs\\pipeline\\builder.py:36\u001b[39m, in \u001b[36mbuild_pipeline_context\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# 1. Load calibration\u001b[39;00m\n\u001b[32m     35\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mLoading calibration from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, config.calibration_path)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m calibration = \u001b[43mload_calibration_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcalibration_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Filter calibration cameras to those with input files/directories\u001b[39;00m\n\u001b[32m     39\u001b[39m input_cameras = \u001b[38;5;28mset\u001b[39m(config.camera_input_map.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\AquaMVS\\src\\aquamvs\\calibration.py:119\u001b[39m, in \u001b[36mload_calibration_data\u001b[39m\u001b[34m(calibration_path)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load AquaCal calibration and convert to PyTorch tensors.\u001b[39;00m\n\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m \u001b[33;03m    ValueError: If calibration file format is invalid.\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Load calibration using AquaCal\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m result = \u001b[43maquacal_load_calibration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalibration_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Convert per-camera data\u001b[39;00m\n\u001b[32m    122\u001b[39m cameras = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\AquaCal\\src\\aquacal\\io\\serialization.py:267\u001b[39m, in \u001b[36mload_calibration\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    264\u001b[39m path = Path(path)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalibration file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    270\u001b[39m     data = json.load(f)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Calibration file not found: calibration.json"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline(config)\n",
    "\n",
    "# Run reconstruction\n",
    "# This will process all frames according to config.preprocessing settings\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "source": [
    "**Expected output:** Per-stage log messages (undistortion, feature matching, triangulation, depth estimation, fusion, surface reconstruction) followed by `\"Pipeline complete\"`. You may also see a benign warning about ring cameras missing input if your dataset does not include all cameras from the calibration file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": [
    "## 3. Examine Intermediate Results\n",
    "\n",
    "The pipeline saves intermediate outputs to the output directory, organized by frame. Let's load and visualize depth maps and consistency maps for frame 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquamvs import load_calibration_data\n",
    "\n",
    "# Path to frame 0 output\n",
    "output = Path(config.output_dir) / \"frame_000000\"\n",
    "\n",
    "# Load calibration to identify ring cameras (auxiliary cameras don't produce depth maps)\n",
    "calibration = load_calibration_data(config.calibration_path)\n",
    "ring_cameras = [c for c in calibration.ring_cameras if c in config.camera_input_map]\n",
    "cam = ring_cameras[0]\n",
    "\n",
    "print(f\"Ring cameras with input: {ring_cameras}\")\n",
    "print(f\"Visualizing outputs for camera: {cam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "source": [
    "### Depth Map\n",
    "\n",
    "Depth maps represent the distance along each ray from the camera to the water surface. Values are in meters (ray depth, not world Z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load depth map (saved as NPZ with 'depth' array)\n",
    "depth_data = np.load(output / \"depth_maps\" / f\"{cam}.npz\")\n",
    "depth = depth_data[\"depth\"]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(depth, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Depth (m)\", shrink=0.8)\n",
    "plt.title(f\"Depth Map - {cam}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "valid_mask = ~np.isnan(depth)\n",
    "print(f\"Depth range: {np.nanmin(depth):.3f} to {np.nanmax(depth):.3f} m\")\n",
    "print(\n",
    "    f\"Valid pixels: {valid_mask.sum()} / {depth.size} ({100 * valid_mask.sum() / depth.size:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {},
   "source": [
    "### Consistency Map\n",
    "\n",
    "Consistency maps indicate how many source cameras agree with the reference camera's depth estimate at each pixel. Higher values (warmer colors) indicate more reliable depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consistency map\n",
    "consistency_data = np.load(output / \"consistency_maps\" / f\"{cam}.npz\")\n",
    "consistency = consistency_data[\"consistency\"]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(consistency, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Consistent views\", shrink=0.8)\n",
    "plt.title(f\"Consistency Map - {cam}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Consistency range: {consistency.min():.0f} to {consistency.max():.0f} views\")\n",
    "print(f\"Mean consistency: {consistency[valid_mask].mean():.1f} views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4641cc4064e0191573fe9c69df29b",
   "metadata": {},
   "source": [
    "## 4. Visualize the Fused Point Cloud\n",
    "\n",
    "The fusion stage merges all camera depth maps into a single 3D point cloud, saved as `fused.ply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309879909854d7188b41380fd92a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Load fused point cloud\n",
    "pcd_path = output / \"point_cloud\" / \"fused.ply\"\n",
    "pcd = o3d.io.read_point_cloud(str(pcd_path))\n",
    "\n",
    "print(f\"Point cloud: {len(pcd.points)} points\")\n",
    "print(f\"Has colors: {pcd.has_colors()}\")\n",
    "print(f\"Has normals: {pcd.has_normals()}\")\n",
    "\n",
    "# Compute bounds\n",
    "bbox = pcd.get_axis_aligned_bounding_box()\n",
    "print(f\"Bounding box: {bbox.get_extent()} m\")\n",
    "\n",
    "# Render an oblique view of the point cloud\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(visible=False, width=1280, height=960)\n",
    "vis.add_geometry(pcd)\n",
    "\n",
    "# Initial render pass to initialize geometry bounds\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "\n",
    "# Set oblique viewpoint (looking from above-front-right)\n",
    "ctr = vis.get_view_control()\n",
    "ctr.set_front([-0.3, -0.5, -0.8])  # oblique: slightly from front-right, mostly above\n",
    "ctr.set_up([0, 0, -1])  # Z-down world: -Z is \"up\" on screen\n",
    "ctr.set_lookat(np.asarray(bbox.get_center()))\n",
    "ctr.set_zoom(0.5)\n",
    "\n",
    "# Second render pass with the updated view\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "img = np.asarray(vis.capture_screen_float_buffer(do_render=True))\n",
    "vis.destroy_window()\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(img)\n",
    "plt.title(\"Fused Point Cloud (oblique view)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed186c9a28b402fb0bc4494df01f08d",
   "metadata": {},
   "source": [
    "**Note:** The above rendering uses Open3D's offscreen renderer, which requires a display (or virtual framebuffer on headless systems). If this cell fails, you can still inspect the point cloud by opening `fused.ply` directly in MeshLab, CloudCompare, or any PLY viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e1581032b452c9409d6c6813c49d1",
   "metadata": {},
   "source": [
    "## 5. Surface Reconstruction and Export\n",
    "\n",
    "The surface reconstruction stage converts the point cloud into a triangle mesh. The default method is Poisson reconstruction, which produces a watertight mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cbbc1e968416e875cc15c1202d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Load reconstructed mesh\n",
    "mesh_path = output / \"mesh\" / \"surface.ply\"\n",
    "mesh = o3d.io.read_triangle_mesh(str(mesh_path))\n",
    "mesh.compute_vertex_normals()\n",
    "\n",
    "print(f\"Mesh: {len(mesh.vertices)} vertices, {len(mesh.triangles)} triangles\")\n",
    "print(f\"Has vertex colors: {mesh.has_vertex_colors()}\")\n",
    "print(f\"Has vertex normals: {mesh.has_vertex_normals()}\")\n",
    "\n",
    "# Render an oblique view of the mesh\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(visible=False, width=1280, height=960)\n",
    "vis.add_geometry(mesh)\n",
    "\n",
    "# Initial render pass to initialize geometry bounds\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "\n",
    "# Set oblique viewpoint (must be set after the first poll/update)\n",
    "ctr = vis.get_view_control()\n",
    "mesh_bbox = mesh.get_axis_aligned_bounding_box()\n",
    "ctr.set_front([-0.3, -0.5, -0.8])\n",
    "ctr.set_up([0, 0, -1])\n",
    "ctr.set_lookat(np.asarray(mesh_bbox.get_center()))\n",
    "ctr.set_zoom(0.5)\n",
    "\n",
    "# Second render pass with the updated view\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "img = np.asarray(vis.capture_screen_float_buffer(do_render=True))\n",
    "vis.destroy_window()\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(img)\n",
    "plt.title(\"Reconstructed Surface Mesh (oblique view)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c27b1587741f2af2001be3712ef0d",
   "metadata": {},
   "source": [
    "### Export to Other Formats\n",
    "\n",
    "AquaMVS provides an `export_mesh` function to convert meshes to OBJ, STL, GLTF, or GLB formats with optional simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b79bc585a40fcaf58bf750017e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquamvs import export_mesh\n",
    "\n",
    "# Export to OBJ (widely supported, preserves colors)\n",
    "obj_path = output / \"surface.obj\"\n",
    "export_mesh(mesh_path, obj_path)\n",
    "print(f\"Exported to OBJ: {obj_path}\")\n",
    "\n",
    "# Export to STL with simplification (for 3D printing)\n",
    "stl_path = output / \"surface_simplified.stl\"\n",
    "export_mesh(mesh_path, stl_path, simplify=10000)\n",
    "print(f\"Exported simplified mesh to STL: {stl_path}\")\n",
    "\n",
    "# Export to GLB (compact, web-ready)\n",
    "glb_path = output / \"surface.glb\"\n",
    "export_mesh(mesh_path, glb_path)\n",
    "print(f\"Exported to GLB: {glb_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916684f9a58a4a2aa5f864670399430d",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have completed a basic reconstruction, explore:\n",
    "\n",
    "- **[CLI Guide](../cli_guide.md)**: Command-line workflow for batch processing\n",
    "- **[Benchmarking Tutorial](benchmark)**: Compare LightGlue and RoMa reconstruction pathways with timing and quality metrics\n",
    "- **[Troubleshooting Guide](../troubleshooting)**: If you encounter issues, see the troubleshooting guide\n",
    "- **[Theory](../theory/index.rst)**: Understand the refractive geometry and algorithms\n",
    "- **[API Reference](../api/index.rst)**: Detailed documentation of all modules and functions\n",
    "\n",
    "### Configuration Tips\n",
    "\n",
    "- **Switch matchers**: Set `matcher_type: \"roma\"` for dense matching (slower, more accurate)\n",
    "- **Adjust depth range**: Modify `reconstruction.depth_min` and `depth_max` to focus on your region of interest\n",
    "- **GPU acceleration**: Set `runtime.device: \"cuda\"` if you have a CUDA-capable GPU\n",
    "- **Quality vs. speed**: Use `aquamvs init --preset fast` when initializing your configuration for a faster but lower-quality reconstruction\n",
    "- **Increase quality**: Increase `reconstruction.num_depths` (default: 64) for higher quality at the cost of longer runtime\n",
    "\n",
    "### Multi-Frame Reconstruction\n",
    "\n",
    "To process multiple frames, adjust `preprocessing` settings in the config:\n",
    "\n",
    "```yaml\n",
    "preprocessing:\n",
    "  frame_start: 0\n",
    "  frame_stop: 100  # Process frames 0-99\n",
    "  frame_step: 10   # Every 10th frame\n",
    "```\n",
    "\n",
    "Each frame's outputs will be saved to `output/frame_XXXXXX/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
