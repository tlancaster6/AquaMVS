{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tlancaster6/AquaMVS/blob/main/docs/tutorial/notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# End-to-End Reconstruction Tutorial\n",
    "\n",
    "This tutorial walks through a complete multi-view stereo reconstruction workflow using the AquaMVS Python API. We start from synchronized multi-camera images, run the reconstruction pipeline, and produce a 3D surface mesh.\n",
    "\n",
    "By the end of this tutorial, you will have:\n",
    "- Loaded and inspected a pipeline configuration\n",
    "- Executed the reconstruction pipeline\n",
    "- Examined intermediate outputs (depth maps, consistency maps)\n",
    "- Visualized the fused point cloud\n",
    "- Exported the final mesh to various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": "# Install AquaMVS (run this cell in Colab; skip locally if already installed)\nimport importlib.util\nimport subprocess\nimport sys\n\nif importlib.util.find_spec(\"aquamvs\") is None:\n    subprocess.run(\n        [\n            sys.executable,\n            \"-m\",\n            \"pip\",\n            \"install\",\n            \"torch\",\n            \"torchvision\",\n            \"--index-url\",\n            \"https://download.pytorch.org/whl/cpu\",\n            \"-q\",\n        ],\n        check=True,\n    )\n    subprocess.run(\n        [\n            sys.executable,\n            \"-m\",\n            \"pip\",\n            \"install\",\n            \"git+https://github.com/cvg/LightGlue.git@edb2b83\",\n            \"git+https://github.com/tlancaster6/RoMaV2.git\",\n            \"aquamvs\",\n            \"-q\",\n        ],\n        check=True,\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_URL = \"https://github.com/tlancaster6/AquaMVS/releases/download/v0.1.0-example-data/aquamvs-example-dataset.zip\"\n",
    "DATASET_DIR = Path(\"aquamvs-example-dataset\")\n",
    "\n",
    "if not DATASET_DIR.exists():\n",
    "    print(\"Downloading example dataset...\")\n",
    "    urllib.request.urlretrieve(DATASET_URL, \"aquamvs-example-dataset.zip\")\n",
    "    with zipfile.ZipFile(\"aquamvs-example-dataset.zip\") as zf:\n",
    "        zf.extractall()\n",
    "    os.remove(\"aquamvs-example-dataset.zip\")\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(f\"Dataset already present at {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from aquamvs import Pipeline, PipelineConfig\n",
    "\n",
    "CONFIG_PATH = Path(\"aquamvs-example-dataset\") / \"config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Configuration\n",
    "\n",
    "The pipeline configuration defines all parameters for reconstruction: camera paths, calibration, feature matching settings, depth estimation parameters, and output options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from YAML\n",
    "config = PipelineConfig.from_yaml(CONFIG_PATH)\n",
    "\n",
    "# Inspect key parameters\n",
    "print(f\"Cameras: {list(config.camera_input_map.keys())}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Matcher type: {config.sparse_matching.matcher_type}\")\n",
    "print(f\"Pipeline mode: {config.reconstruction.pipeline_mode}\")\n",
    "print(f\"Device: {config.runtime.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "**Expected output:** List of camera names (e.g., `['e3v82e0', 'e3v82e1', ...]`), output directory path, matcher type (`'lightglue'` or `'roma'`), pipeline mode (`'full'` or `'sparse'`), and device (`'cpu'` or `'cuda'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "## 2. Run the Pipeline\n",
    "\n",
    "The `Pipeline` class provides the primary programmatic interface. Calling `.run()` executes the full reconstruction workflow:\n",
    "\n",
    "1. **Undistortion**: Apply camera calibration to remove lens distortion\n",
    "2. **Feature Matching**: Extract and match features across camera pairs (LightGlue or RoMa)\n",
    "3. **Triangulation**: Compute 3D points from feature correspondences (sparse mode) or...\n",
    "4. **Plane Sweep Stereo**: Dense depth estimation via photometric cost volume (full mode)\n",
    "5. **Depth Fusion**: Merge multi-view depth maps into a single point cloud\n",
    "6. **Surface Reconstruction**: Generate a triangle mesh from the point cloud\n",
    "\n",
    "This may take several minutes depending on image resolution, number of cameras, and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = Pipeline(config)\n",
    "\n",
    "# Run reconstruction\n",
    "# This will process all frames according to config.preprocessing settings\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "source": [
    "**Expected output:** Progress bars and log messages indicating stage completion. Final message: `\"Pipeline complete\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": [
    "## 3. Examine Intermediate Results\n",
    "\n",
    "The pipeline saves intermediate outputs to the output directory, organized by frame. Let's load and visualize depth maps and consistency maps for frame 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to frame 0 output\n",
    "output = Path(config.output_dir) / \"frame_000000\"\n",
    "\n",
    "# Pick the first camera to visualize\n",
    "camera_names = list(config.camera_input_map.keys())\n",
    "cam = camera_names[0]\n",
    "\n",
    "print(f\"Visualizing outputs for camera: {cam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "source": [
    "### Depth Map\n",
    "\n",
    "Depth maps represent the distance along each ray from the camera to the water surface. Values are in meters (ray depth, not world Z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load depth map (saved as NPZ with 'depth' array)\n",
    "depth_data = np.load(output / \"depth_maps\" / f\"{cam}.npz\")\n",
    "depth = depth_data[\"depth\"]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(depth, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Depth (m)\", shrink=0.8)\n",
    "plt.title(f\"Depth Map \u00e2\u20ac\u201d {cam}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "valid_mask = ~np.isnan(depth)\n",
    "print(f\"Depth range: {np.nanmin(depth):.3f} to {np.nanmax(depth):.3f} m\")\n",
    "print(\n",
    "    f\"Valid pixels: {valid_mask.sum()} / {depth.size} ({100 * valid_mask.sum() / depth.size:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {},
   "source": [
    "### Consistency Map\n",
    "\n",
    "Consistency maps indicate how many source cameras agree with the reference camera's depth estimate at each pixel. Higher values (warmer colors) indicate more reliable depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consistency map\n",
    "consistency_data = np.load(output / \"consistency_maps\" / f\"{cam}.npz\")\n",
    "consistency = consistency_data[\"consistency\"]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(consistency, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Consistent views\", shrink=0.8)\n",
    "plt.title(f\"Consistency Map \u00e2\u20ac\u201d {cam}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Consistency range: {consistency.min():.0f} to {consistency.max():.0f} views\")\n",
    "print(f\"Mean consistency: {consistency[valid_mask].mean():.1f} views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4641cc4064e0191573fe9c69df29b",
   "metadata": {},
   "source": [
    "## 4. Visualize the Fused Point Cloud\n",
    "\n",
    "The fusion stage merges all camera depth maps into a single 3D point cloud, saved as `fused.ply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309879909854d7188b41380fd92a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Load fused point cloud\n",
    "pcd_path = output / \"point_cloud\" / \"fused.ply\"\n",
    "pcd = o3d.io.read_point_cloud(str(pcd_path))\n",
    "\n",
    "print(f\"Point cloud: {len(pcd.points)} points\")\n",
    "print(f\"Has colors: {pcd.has_colors()}\")\n",
    "print(f\"Has normals: {pcd.has_normals()}\")\n",
    "\n",
    "# Compute bounds\n",
    "bbox = pcd.get_axis_aligned_bounding_box()\n",
    "print(f\"Bounding box: {bbox.get_extent()} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed186c9a28b402fb0bc4494df01f08d",
   "metadata": {},
   "source": [
    "**Note on visualization:** `o3d.visualization.draw_geometries(pcd)` requires a display (X server on Linux, windowed environment). In Jupyter, you can:\n",
    "\n",
    "- Use Open3D's Jupyter visualizer: `o3d.visualization.draw(pcd)` (requires `open3d>=0.16`)\n",
    "- Export to HTML for web-based viewing\n",
    "- Save a rendered image for display\n",
    "\n",
    "For headless environments, skip interactive visualization and proceed to mesh export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e1581032b452c9409d6c6813c49d1",
   "metadata": {},
   "source": [
    "## 5. Surface Reconstruction and Export\n",
    "\n",
    "The surface reconstruction stage converts the point cloud into a triangle mesh. The default method is Poisson reconstruction, which produces a watertight mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cbbc1e968416e875cc15c1202d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reconstructed mesh\n",
    "mesh_path = output / \"mesh\" / \"surface.ply\"\n",
    "mesh = o3d.io.read_triangle_mesh(str(mesh_path))\n",
    "\n",
    "print(f\"Mesh: {len(mesh.vertices)} vertices, {len(mesh.triangles)} triangles\")\n",
    "print(f\"Has vertex colors: {mesh.has_vertex_colors()}\")\n",
    "print(f\"Has vertex normals: {mesh.has_vertex_normals()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c27b1587741f2af2001be3712ef0d",
   "metadata": {},
   "source": [
    "### Export to Other Formats\n",
    "\n",
    "AquaMVS provides an `export_mesh` function to convert meshes to OBJ, STL, GLTF, or GLB formats with optional simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b79bc585a40fcaf58bf750017e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquamvs import export_mesh\n",
    "\n",
    "# Export to OBJ (widely supported, preserves colors)\n",
    "obj_path = output / \"surface.obj\"\n",
    "export_mesh(str(mesh_path), str(obj_path))\n",
    "print(f\"Exported to OBJ: {obj_path}\")\n",
    "\n",
    "# Export to STL with simplification (for 3D printing)\n",
    "stl_path = output / \"surface_simplified.stl\"\n",
    "export_mesh(str(mesh_path), str(stl_path), simplify=10000)\n",
    "print(f\"Exported simplified mesh to STL: {stl_path}\")\n",
    "\n",
    "# Export to GLB (compact, web-ready)\n",
    "glb_path = output / \"surface.glb\"\n",
    "export_mesh(str(mesh_path), str(glb_path))\n",
    "print(f\"Exported to GLB: {glb_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916684f9a58a4a2aa5f864670399430d",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have completed a basic reconstruction, explore:\n",
    "\n",
    "- **[CLI Guide](../cli_guide.md)**: Command-line workflow for batch processing\n",
    "- **[Benchmarking Tutorial](benchmark)**: Compare LightGlue and RoMa reconstruction pathways with timing and quality metrics\n",
    "- **[Troubleshooting Guide](../troubleshooting)**: If you encounter issues, see the troubleshooting guide\n",
    "- **[Theory](../theory/index.rst)**: Understand the refractive geometry and algorithms\n",
    "- **[API Reference](../api/index.rst)**: Detailed documentation of all modules and functions\n",
    "\n",
    "### Configuration Tips\n",
    "\n",
    "- **Switch matchers**: Set `sparse_matching.matcher_type: \"roma\"` for dense matching (slower, more accurate)\n",
    "- **Adjust depth range**: Modify `reconstruction.depth_min` and `depth_max` to focus on your region of interest\n",
    "- **GPU acceleration**: Set `runtime.device: \"cuda\"` if you have a CUDA-capable GPU\n",
    "- **Quality vs. speed**: Use `aquamvs init --preset fast` when initializing your configuration for a faster but lower-quality reconstruction\n",
    "- **Increase quality**: Increase `reconstruction.num_depth_hypotheses` (default: 64) for higher quality at the cost of longer runtime\n",
    "\n",
    "### Multi-Frame Reconstruction\n",
    "\n",
    "To process multiple frames, adjust `preprocessing` settings in the config:\n",
    "\n",
    "```yaml\n",
    "preprocessing:\n",
    "  frame_start: 0\n",
    "  frame_stop: 100  # Process frames 0-99\n",
    "  frame_step: 10   # Every 10th frame\n",
    "```\n",
    "\n",
    "Each frame's outputs will be saved to `output/frame_XXXXXX/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
