{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Reconstruction Tutorial\n",
    "\n",
    "This tutorial walks through a complete multi-view stereo reconstruction workflow using the AquaMVS Python API. We'll start from synchronized multi-camera images, run the reconstruction pipeline, and produce a 3D surface mesh.\n",
    "\n",
    "By the end of this tutorial, you'll have:\n",
    "- Loaded and inspected a pipeline configuration\n",
    "- Executed the reconstruction pipeline\n",
    "- Examined intermediate outputs (depth maps, consistency maps)\n",
    "- Visualized the fused point cloud\n",
    "- Exported the final mesh to various formats\n",
    "\n",
    "## Example Dataset\n",
    "\n",
    "This tutorial assumes you have downloaded the example dataset. If you haven't already:\n",
    "\n",
    "1. Download from: [https://github.com/tlancaster6/AquaMVS/releases/download/v0.1.0-example-data/aquamvs-example-dataset.zip](https://github.com/tlancaster6/AquaMVS/releases/download/v0.1.0-example-data/aquamvs-example-dataset.zip)\n",
    "2. Extract to a local directory\n",
    "3. The dataset includes:\n",
    "   - Synchronized camera images (or videos)\n",
    "   - AquaCal calibration JSON\n",
    "   - Pre-configured `config.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Ensure you have installed AquaMVS and its dependencies:\n",
    "\n",
    "```bash\n",
    "# Install PyTorch (CPU or CUDA version depending on your hardware)\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install AquaMVS\n",
    "pip install aquamvs\n",
    "\n",
    "# Install optional dependencies (LightGlue, RoMa)\n",
    "pip install git+https://github.com/cvg/LightGlue.git@edb2b83\n",
    "pip install git+https://github.com/tlancaster6/RoMaV2.git\n",
    "```\n",
    "\n",
    "See the [Installation Guide](../installation.rst) for detailed instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from aquamvs import Pipeline, PipelineConfig\n",
    "\n",
    "# Point to example dataset (adjust path as needed)\n",
    "DATA_DIR = Path(\"example_data\")\n",
    "CONFIG_PATH = DATA_DIR / \"config.yaml\"\n",
    "\n",
    "# Verify the config exists\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Config file not found at {CONFIG_PATH}. \"\n",
    "        \"Download the example dataset or adjust DATA_DIR.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Configuration\n",
    "\n",
    "The pipeline configuration defines all parameters for reconstruction: camera paths, calibration, feature matching settings, depth estimation parameters, and output options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from YAML\n",
    "config = PipelineConfig.from_yaml(CONFIG_PATH)\n",
    "\n",
    "# Inspect key parameters\n",
    "print(f\"Cameras: {list(config.camera_video_map.keys())}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Matcher type: {config.sparse_matching.matcher_type}\")\n",
    "print(f\"Pipeline mode: {config.reconstruction.pipeline_mode}\")\n",
    "print(f\"Device: {config.runtime.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:** List of camera names (e.g., `['e3v82e0', 'e3v82e1', ...]`), output directory path, matcher type (`'lightglue'` or `'roma'`), pipeline mode (`'full'` or `'sparse'`), and device (`'cpu'` or `'cuda'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run the Pipeline\n",
    "\n",
    "The `Pipeline` class provides the primary programmatic interface. Calling `.run()` executes the full reconstruction workflow:\n",
    "\n",
    "1. **Undistortion**: Apply camera calibration to remove lens distortion\n",
    "2. **Feature Matching**: Extract and match features across camera pairs (LightGlue or RoMa)\n",
    "3. **Triangulation**: Compute 3D points from feature correspondences (sparse mode) or...\n",
    "4. **Plane Sweep Stereo**: Dense depth estimation via photometric cost volume (full mode)\n",
    "5. **Depth Fusion**: Merge multi-view depth maps into a single point cloud\n",
    "6. **Surface Reconstruction**: Generate a triangle mesh from the point cloud\n",
    "\n",
    "This may take several minutes depending on image resolution, number of cameras, and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = Pipeline(config)\n",
    "\n",
    "# Run reconstruction\n",
    "# This will process all frames according to config.preprocessing settings\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:** Progress bars and log messages indicating stage completion. Final message: `\"Pipeline complete\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examine Intermediate Results\n",
    "\n",
    "The pipeline saves intermediate outputs to the output directory, organized by frame. Let's load and visualize depth maps and consistency maps for frame 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to frame 0 output\n",
    "output = Path(config.output_dir) / \"frame_000000\"\n",
    "\n",
    "# Pick the first camera to visualize\n",
    "camera_names = list(config.camera_video_map.keys())\n",
    "cam = camera_names[0]\n",
    "\n",
    "print(f\"Visualizing outputs for camera: {cam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth Map\n",
    "\n",
    "Depth maps represent the distance along each ray from the camera to the water surface. Values are in meters (ray depth, not world Z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load depth map (saved as NPZ with 'depth' array)\n",
    "depth_data = np.load(output / f\"{cam}_depth.npz\")\n",
    "depth = depth_data[\"depth\"]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(depth, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Depth (m)\", shrink=0.8)\n",
    "plt.title(f\"Depth Map — {cam}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "valid_mask = ~np.isnan(depth)\n",
    "print(f\"Depth range: {np.nanmin(depth):.3f} to {np.nanmax(depth):.3f} m\")\n",
    "print(\n",
    "    f\"Valid pixels: {valid_mask.sum()} / {depth.size} ({100 * valid_mask.sum() / depth.size:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency Map\n",
    "\n",
    "Consistency maps indicate how many source cameras agree with the reference camera's depth estimate at each pixel. Higher values (warmer colors) indicate more reliable depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consistency map\n",
    "consistency_data = np.load(output / f\"{cam}_consistency.npz\")\n",
    "consistency = consistency_data[\"consistency\"]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(consistency, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Consistent views\", shrink=0.8)\n",
    "plt.title(f\"Consistency Map — {cam}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Consistency range: {consistency.min():.0f} to {consistency.max():.0f} views\")\n",
    "print(f\"Mean consistency: {consistency[valid_mask].mean():.1f} views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the Fused Point Cloud\n",
    "\n",
    "The fusion stage merges all camera depth maps into a single 3D point cloud, saved as `fused.ply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Load fused point cloud\n",
    "pcd_path = output / \"fused.ply\"\n",
    "pcd = o3d.io.read_point_cloud(str(pcd_path))\n",
    "\n",
    "print(f\"Point cloud: {len(pcd.points)} points\")\n",
    "print(f\"Has colors: {pcd.has_colors()}\")\n",
    "print(f\"Has normals: {pcd.has_normals()}\")\n",
    "\n",
    "# Compute bounds\n",
    "bbox = pcd.get_axis_aligned_bounding_box()\n",
    "print(f\"Bounding box: {bbox.get_extent()} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on visualization:** `o3d.visualization.draw_geometries(pcd)` requires a display (X server on Linux, windowed environment). In Jupyter, you can:\n",
    "\n",
    "- Use Open3D's Jupyter visualizer: `o3d.visualization.draw(pcd)` (requires `open3d>=0.16`)\n",
    "- Export to HTML for web-based viewing\n",
    "- Save a rendered image for display\n",
    "\n",
    "For headless environments, skip interactive visualization and proceed to mesh export."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Surface Reconstruction and Export\n",
    "\n",
    "The surface reconstruction stage converts the point cloud into a triangle mesh. The default method is Poisson reconstruction, which produces a watertight mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reconstructed mesh\n",
    "mesh_path = output / \"surface.ply\"\n",
    "mesh = o3d.io.read_triangle_mesh(str(mesh_path))\n",
    "\n",
    "print(f\"Mesh: {len(mesh.vertices)} vertices, {len(mesh.triangles)} triangles\")\n",
    "print(f\"Has vertex colors: {mesh.has_vertex_colors()}\")\n",
    "print(f\"Has vertex normals: {mesh.has_vertex_normals()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to Other Formats\n",
    "\n",
    "AquaMVS provides an `export_mesh` function to convert meshes to OBJ, STL, GLTF, or GLB formats with optional simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquamvs import export_mesh\n",
    "\n",
    "# Export to OBJ (widely supported, preserves colors)\n",
    "obj_path = output / \"surface.obj\"\n",
    "export_mesh(str(mesh_path), str(obj_path))\n",
    "print(f\"Exported to OBJ: {obj_path}\")\n",
    "\n",
    "# Export to STL with simplification (for 3D printing)\n",
    "stl_path = output / \"surface_simplified.stl\"\n",
    "export_mesh(str(mesh_path), str(stl_path), simplify=10000)\n",
    "print(f\"Exported simplified mesh to STL: {stl_path}\")\n",
    "\n",
    "# Export to GLB (compact, web-ready)\n",
    "glb_path = output / \"surface.glb\"\n",
    "export_mesh(str(mesh_path), str(glb_path))\n",
    "print(f\"Exported to GLB: {glb_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've completed a basic reconstruction, explore:\n",
    "\n",
    "- **[CLI Guide](../cli_guide.md)**: Command-line workflow for batch processing\n",
    "- **[Theory](../theory/index.rst)**: Understand the refractive geometry and algorithms\n",
    "- **[API Reference](../api/index.rst)**: Detailed documentation of all modules and functions\n",
    "\n",
    "### Configuration Tips\n",
    "\n",
    "- **Switch matchers**: Set `sparse_matching.matcher_type: \"roma\"` for dense matching (slower, more accurate)\n",
    "- **Adjust depth range**: Modify `reconstruction.depth_min` and `depth_max` to focus on your region of interest\n",
    "- **GPU acceleration**: Set `runtime.device: \"cuda\"` if you have a CUDA-capable GPU\n",
    "- **Quality vs. speed**: Increase `reconstruction.num_depth_hypotheses` (default: 64) for higher quality at the cost of longer runtime\n",
    "\n",
    "### Multi-Frame Reconstruction\n",
    "\n",
    "To process multiple frames, adjust `preprocessing` settings in the config:\n",
    "\n",
    "```yaml\n",
    "preprocessing:\n",
    "  frame_start: 0\n",
    "  frame_stop: 100  # Process frames 0-99\n",
    "  frame_step: 10   # Every 10th frame\n",
    "```\n",
    "\n",
    "Each frame's outputs will be saved to `output/frame_XXXXXX/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
