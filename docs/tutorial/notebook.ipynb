{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tlancaster6/AquaMVS/blob/main/docs/tutorial/notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# End-to-End Reconstruction Tutorial\n",
    "\n",
    "This tutorial walks through a complete multi-view stereo reconstruction workflow using the AquaMVS Python API. We start from synchronized multi-camera images, run the reconstruction pipeline, and produce a 3D surface mesh.\n",
    "\n",
    "By the end of this tutorial, you will have:\n",
    "- Loaded and inspected a pipeline configuration\n",
    "- Executed the reconstruction pipeline\n",
    "- Examined intermediate outputs (depth maps, consistency maps)\n",
    "- Visualized the fused point cloud\n",
    "- Exported the final mesh to various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AquaMVS (run this cell in Colab; skip locally if already installed)\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if importlib.util.find_spec(\"aquamvs\") is None:\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"torch\",\n",
    "            \"torchvision\",\n",
    "            \"--index-url\",\n",
    "            \"https://download.pytorch.org/whl/cpu\",\n",
    "            \"-q\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"git+https://github.com/cvg/LightGlue.git@edb2b83\",\n",
    "            \"git+https://github.com/tlancaster6/RoMaV2.git\",\n",
    "            \"aquamvs\",\n",
    "            \"-q\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already present at aquamvs-example-dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_URL = \"https://github.com/tlancaster6/AquaMVS/releases/download/v0.1.0-example-data/aquamvs-example-dataset.zip\"\n",
    "DATASET_DIR = Path(\"aquamvs-example-dataset\")\n",
    "\n",
    "if not DATASET_DIR.exists():\n",
    "    print(\"Downloading example dataset...\")\n",
    "    urllib.request.urlretrieve(DATASET_URL, \"aquamvs-example-dataset.zip\")\n",
    "    with zipfile.ZipFile(\"aquamvs-example-dataset.zip\") as zf:\n",
    "        zf.extractall(DATASET_DIR)\n",
    "    os.remove(\"aquamvs-example-dataset.zip\")\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(f\"Dataset already present at {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from aquamvs import Pipeline, PipelineConfig\n",
    "\n",
    "# Enable logging so pipeline stages print progress\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(name)s - %(message)s\")\n",
    "\n",
    "CONFIG_PATH = Path(\"aquamvs-example-dataset\") / \"config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Configuration\n",
    "\n",
    "The pipeline configuration defines all parameters for reconstruction: camera paths, calibration, feature matching settings, depth estimation parameters, and output options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameras: ['e3v8250', 'e3v829d', 'e3v82e0', 'e3v831e', 'e3v832e', 'e3v8334', 'e3v83e9', 'e3v83eb', 'e3v83ee', 'e3v83ef', 'e3v83f0', 'e3v83f1']\n",
      "Output directory: aquamvs-example-dataset/output\n",
      "Extractor type: superpoint\n",
      "Pipeline mode: full\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from YAML\n",
    "config = PipelineConfig.from_yaml(CONFIG_PATH)\n",
    "\n",
    "# Inspect key parameters\n",
    "print(f\"Cameras: {list(config.camera_input_map.keys())}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Extractor type: {config.sparse_matching.extractor_type}\")\n",
    "print(f\"Pipeline mode: {config.pipeline_mode}\")\n",
    "print(f\"Device: {config.runtime.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "**Expected output:** List of camera names (e.g., `['e3v82e0', 'e3v82e1', ...]`), output directory path, extractor type (`'superpoint'`), pipeline mode (`'full'` or `'sparse'`), and device (`'cpu'` or `'cuda'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "## 2. Run the Pipeline\n",
    "\n",
    "The `Pipeline` class provides the primary programmatic interface. Calling `.run()` executes the full reconstruction workflow:\n",
    "\n",
    "1. **Undistortion**: Apply camera calibration to remove lens distortion\n",
    "2. **Feature Matching**: Extract and match features across camera pairs (LightGlue or RoMa)\n",
    "3. **Triangulation**: Compute 3D points from feature correspondences (sparse mode) or...\n",
    "4. **Plane Sweep Stereo**: Dense depth estimation via photometric cost volume (full mode)\n",
    "5. **Depth Fusion**: Merge multi-view depth maps into a single point cloud\n",
    "6. **Surface Reconstruction**: Generate a triangle mesh from the point cloud\n",
    "\n",
    "This step can take a long time to run, depending on image resolution, number of cameras, and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aquamvs.pipeline.builder - Loading calibration from aquamvs-example-dataset/calibration.json\n",
      "aquamvs.pipeline.builder - Ring cameras in calibration but missing input (skipped): ['e3v82f9']\n",
      "aquamvs.pipeline.builder - Found 11 ring cameras, 1 auxiliary cameras (of 12/1 in calibration)\n",
      "aquamvs.pipeline.builder - Computing undistortion maps\n",
      "aquamvs.pipeline.builder - Creating projection models\n",
      "aquamvs.pipeline.builder - Selecting camera pairs\n",
      "aquamvs.pipeline.builder - Config saved to aquamvs-example-dataset\\output\\config.yaml\n",
      "aquamvs.pipeline.runner - Detected image directory input\n",
      "aquamvs.io - Detected 1 frames across 12 cameras (image directory input)\n",
      "aquamvs.pipeline.runner - Processing frames 0 to end (step 1)\n",
      "aquamvs.pipeline.stages.undistortion - Frame 0: undistorting images\n",
      "aquamvs.pipeline.stages.undistortion - undistortion: 48.3 ms\n",
      "aquamvs.pipeline.stages.dense_matching - Frame 0: running RoMa v2 dense matching (full mode)\n",
      "Using cache found in C:\\Users\\tucke/.cache\\torch\\hub\\facebookresearch_dinov3_adc254450203739c8149213a7a69d8d905b4fcfa\n",
      "dinov3 - using base=100 for rope new\n",
      "dinov3 - using min_period=None for rope new\n",
      "dinov3 - using max_period=None for rope new\n",
      "dinov3 - using normalize_coords=separate for rope new\n",
      "dinov3 - using shift_coords=None for rope new\n",
      "dinov3 - using rescale_coords=2 for rope new\n",
      "dinov3 - using jitter_coords=None for rope new\n",
      "dinov3 - using dtype=fp32 for rope new\n",
      "dinov3 - using mlp layer as FFN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2026-02-17 21:10:05 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">romav2</span>.romav2 - romav<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">2:116</span> in __init__ - RoMa v2 initialized.                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2026-02-17 21:10:05\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36mromav2\u001b[0m.romav2 - romav\u001b[1;92m2:116\u001b[0m in __init__ - RoMa v2 initialized.                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aquamvs.pipeline.stages.dense_matching - Frame 0: converting RoMa warps to depth maps\n",
      "aquamvs.pipeline.stages.dense_matching - dense_matching: 716815.0 ms\n",
      "aquamvs.pipeline.stages.fusion - Frame 0: skipping geometric consistency filter (RoMa path)\n",
      "aquamvs.pipeline.stages.fusion - Frame 0: fusing depth maps\n",
      "aquamvs.pipeline.stages.fusion - Frame 0: removed 548933 outliers (4.4%) from fused cloud\n",
      "aquamvs.pipeline.stages.fusion - fusion: 130389.7 ms\n",
      "aquamvs.pipeline.stages.surface - Frame 0: reconstructing surface\n",
      "aquamvs.pipeline.stages.surface - surface_reconstruction: 74898.2 ms\n",
      "aquamvs.pipeline.runner - Frame 0: complete\n",
      "aquamvs.pipeline.runner - Pipeline complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline(config)\n",
    "\n",
    "# Run reconstruction\n",
    "# This will process all frames according to config.preprocessing settings\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "source": [
    "**Expected output:** Per-stage log messages (undistortion, feature matching, triangulation, depth estimation, fusion, surface reconstruction) followed by `\"Pipeline complete\"`. You may also see a benign warning about ring cameras missing input if your dataset does not include all cameras from the calibration file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": [
    "## 3. Examine Intermediate Results\n",
    "\n",
    "The pipeline saves intermediate outputs to the output directory, organized by frame. Let's load and visualize depth maps and consistency maps for frame 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquamvs import load_calibration_data\n",
    "\n",
    "# Path to frame 0 output\n",
    "output = Path(config.output_dir) / \"frame_000000\"\n",
    "\n",
    "# Load calibration to identify ring cameras (auxiliary cameras don't produce depth maps)\n",
    "calibration = load_calibration_data(config.calibration_path)\n",
    "ring_cameras = [c for c in calibration.ring_cameras if c in config.camera_input_map]\n",
    "cam = ring_cameras[0]\n",
    "\n",
    "print(f\"Ring cameras with input: {ring_cameras}\")\n",
    "print(f\"Visualizing outputs for camera: {cam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "source": [
    "### Depth Map\n",
    "\n",
    "Depth maps represent the distance along each ray from the camera to the water surface. Values are in meters (ray depth, not world Z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load depth map (saved as NPZ with 'depth' array)\n",
    "depth_data = np.load(output / \"depth_maps\" / f\"{cam}.npz\")\n",
    "depth = depth_data[\"depth\"]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(depth, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Depth (m)\", shrink=0.8)\n",
    "plt.title(f\"Depth Map - {cam}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "valid_mask = ~np.isnan(depth)\n",
    "print(f\"Depth range: {np.nanmin(depth):.3f} to {np.nanmax(depth):.3f} m\")\n",
    "print(\n",
    "    f\"Valid pixels: {valid_mask.sum()} / {depth.size} ({100 * valid_mask.sum() / depth.size:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {},
   "source": [
    "### Consistency Map\n",
    "\n",
    "Consistency maps indicate how many source cameras agree with the reference camera's depth estimate at each pixel. Higher values (warmer colors) indicate more reliable depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consistency map\n",
    "consistency_data = np.load(output / \"consistency_maps\" / f\"{cam}.npz\")\n",
    "consistency = consistency_data[\"consistency\"]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(consistency, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Consistent views\", shrink=0.8)\n",
    "plt.title(f\"Consistency Map - {cam}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Consistency range: {consistency.min():.0f} to {consistency.max():.0f} views\")\n",
    "print(f\"Mean consistency: {consistency[valid_mask].mean():.1f} views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4641cc4064e0191573fe9c69df29b",
   "metadata": {},
   "source": [
    "## 4. Visualize the Fused Point Cloud\n",
    "\n",
    "The fusion stage merges all camera depth maps into a single 3D point cloud, saved as `fused.ply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309879909854d7188b41380fd92a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Load fused point cloud\n",
    "pcd_path = output / \"point_cloud\" / \"fused.ply\"\n",
    "pcd = o3d.io.read_point_cloud(str(pcd_path))\n",
    "\n",
    "print(f\"Point cloud: {len(pcd.points)} points\")\n",
    "print(f\"Has colors: {pcd.has_colors()}\")\n",
    "print(f\"Has normals: {pcd.has_normals()}\")\n",
    "\n",
    "# Compute bounds\n",
    "bbox = pcd.get_axis_aligned_bounding_box()\n",
    "print(f\"Bounding box: {bbox.get_extent()} m\")\n",
    "\n",
    "# Render an oblique view of the point cloud\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(visible=False, width=1280, height=960)\n",
    "vis.add_geometry(pcd)\n",
    "\n",
    "# Initial render pass to initialize geometry bounds\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "\n",
    "# Set oblique viewpoint (looking from above-front-right)\n",
    "ctr = vis.get_view_control()\n",
    "ctr.set_front([-0.3, -0.5, -0.8])  # oblique: slightly from front-right, mostly above\n",
    "ctr.set_up([0, 0, -1])  # Z-down world: -Z is \"up\" on screen\n",
    "ctr.set_lookat(np.asarray(bbox.get_center()))\n",
    "ctr.set_zoom(0.5)\n",
    "\n",
    "# Second render pass with the updated view\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "img = np.asarray(vis.capture_screen_float_buffer(do_render=True))\n",
    "vis.destroy_window()\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(img)\n",
    "plt.title(\"Fused Point Cloud (oblique view)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed186c9a28b402fb0bc4494df01f08d",
   "metadata": {},
   "source": [
    "**Note:** The above rendering uses Open3D's offscreen renderer, which requires a display (or virtual framebuffer on headless systems). If this cell fails, you can still inspect the point cloud by opening `fused.ply` directly in MeshLab, CloudCompare, or any PLY viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e1581032b452c9409d6c6813c49d1",
   "metadata": {},
   "source": [
    "## 5. Surface Reconstruction and Export\n",
    "\n",
    "The surface reconstruction stage converts the point cloud into a triangle mesh. The default method is Poisson reconstruction, which produces a watertight mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cbbc1e968416e875cc15c1202d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Load reconstructed mesh\n",
    "mesh_path = output / \"mesh\" / \"surface.ply\"\n",
    "mesh = o3d.io.read_triangle_mesh(str(mesh_path))\n",
    "mesh.compute_vertex_normals()\n",
    "\n",
    "print(f\"Mesh: {len(mesh.vertices)} vertices, {len(mesh.triangles)} triangles\")\n",
    "print(f\"Has vertex colors: {mesh.has_vertex_colors()}\")\n",
    "print(f\"Has vertex normals: {mesh.has_vertex_normals()}\")\n",
    "\n",
    "# Render an oblique view of the mesh\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(visible=False, width=1280, height=960)\n",
    "vis.add_geometry(mesh)\n",
    "\n",
    "# Initial render pass to initialize geometry bounds\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "\n",
    "# Set oblique viewpoint (must be set after the first poll/update)\n",
    "ctr = vis.get_view_control()\n",
    "mesh_bbox = mesh.get_axis_aligned_bounding_box()\n",
    "ctr.set_front([-0.3, -0.5, -0.8])\n",
    "ctr.set_up([0, 0, -1])\n",
    "ctr.set_lookat(np.asarray(mesh_bbox.get_center()))\n",
    "ctr.set_zoom(0.5)\n",
    "\n",
    "# Second render pass with the updated view\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "img = np.asarray(vis.capture_screen_float_buffer(do_render=True))\n",
    "vis.destroy_window()\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(img)\n",
    "plt.title(\"Reconstructed Surface Mesh (oblique view)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c27b1587741f2af2001be3712ef0d",
   "metadata": {},
   "source": [
    "### Export to Other Formats\n",
    "\n",
    "AquaMVS provides an `export_mesh` function to convert meshes to OBJ, STL, GLTF, or GLB formats with optional simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b79bc585a40fcaf58bf750017e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquamvs import export_mesh\n",
    "\n",
    "# Export to OBJ (widely supported, preserves colors)\n",
    "obj_path = output / \"surface.obj\"\n",
    "export_mesh(mesh_path, obj_path)\n",
    "print(f\"Exported to OBJ: {obj_path}\")\n",
    "\n",
    "# Export to STL with simplification (for 3D printing)\n",
    "stl_path = output / \"surface_simplified.stl\"\n",
    "export_mesh(mesh_path, stl_path, simplify=10000)\n",
    "print(f\"Exported simplified mesh to STL: {stl_path}\")\n",
    "\n",
    "# Export to GLB (compact, web-ready)\n",
    "glb_path = output / \"surface.glb\"\n",
    "export_mesh(mesh_path, glb_path)\n",
    "print(f\"Exported to GLB: {glb_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916684f9a58a4a2aa5f864670399430d",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have completed a basic reconstruction, explore:\n",
    "\n",
    "- **[CLI Guide](../cli_guide.md)**: Command-line workflow for batch processing\n",
    "- **[Benchmarking Tutorial](benchmark)**: Compare LightGlue and RoMa reconstruction pathways with timing and quality metrics\n",
    "- **[Troubleshooting Guide](../troubleshooting)**: If you encounter issues, see the troubleshooting guide\n",
    "- **[Theory](../theory/index.rst)**: Understand the refractive geometry and algorithms\n",
    "- **[API Reference](../api/index.rst)**: Detailed documentation of all modules and functions\n",
    "\n",
    "### Configuration Tips\n",
    "\n",
    "- **Switch matchers**: Set `matcher_type: \"roma\"` for dense matching (slower, more accurate)\n",
    "- **Adjust depth range**: Modify `reconstruction.depth_min` and `depth_max` to focus on your region of interest\n",
    "- **GPU acceleration**: Set `runtime.device: \"cuda\"` if you have a CUDA-capable GPU\n",
    "- **Quality vs. speed**: Use `aquamvs init --preset fast` when initializing your configuration for a faster but lower-quality reconstruction\n",
    "- **Increase quality**: Increase `reconstruction.num_depths` (default: 64) for higher quality at the cost of longer runtime\n",
    "\n",
    "### Multi-Frame Reconstruction\n",
    "\n",
    "To process multiple frames, adjust `preprocessing` settings in the config:\n",
    "\n",
    "```yaml\n",
    "preprocessing:\n",
    "  frame_start: 0\n",
    "  frame_stop: 100  # Process frames 0-99\n",
    "  frame_step: 10   # Every 10th frame\n",
    "```\n",
    "\n",
    "Each frame's outputs will be saved to `output/frame_XXXXXX/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
